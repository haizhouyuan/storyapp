Skip to content
Open sidebar
You said:
github actions workflow éƒ½èƒ½æ€ä¹ˆè§¦å‘ï¼Œworkflow ä¹‹é—´å¯ä»¥ç›¸äº’è§¦å‘åƒé’©å­ä¸€ç±»çš„åŠŸèƒ½å—ï¼Œæˆ‘ç°åœ¨æƒ³æœ¬åœ°å®¹å™¨æµ‹è¯•é€šè¿‡åå°±å‘prï¼Œç„¶åå‰©ä¸‹çš„prè¯„å®¡éƒ¨ç½²æµ‹è¯•ï¼Œä¿®å¤å†è¯„å®¡ç›´åˆ°é€šè¿‡çš„æ‰€æœ‰æµç¨‹éƒ½ç”¨codexå’Œclaudecodeå®Œæˆã€‚ä½ çœ‹çœ‹å¦‚ä½•å®ç°ï¼Œæˆ‘å·²ç»æœ‰å¾ˆå¤šworkflowäº†ï¼Œä½ çœ‹ä¸€ä¸‹æˆ‘å·²ç»å¼€å¯äº†connectorã€‚
ChatGPT said:

Thought for 1m 42s
å¥½çš„ï¼æˆ‘å…ˆæŠŠä¸¤ä¸ªé—®é¢˜è®²æ¸…æ¥šï¼Œç„¶åç»™ä½ ä¸€å¥—ã€Œæœ¬åœ°å®¹å™¨æµ‹è¿‡â†’è‡ªåŠ¨å¼€ PRâ†’AI è¯„å®¡â†’éƒ¨ç½²åˆ°æµ‹è¯•â†’å¾ªç¯ä¿®å¤ç›´åˆ°é€šè¿‡â†’åˆå¹¶ã€çš„è½åœ°æ–¹æ¡ˆï¼ˆç›´æ¥ç»™å¯ç”¨ YAML å’Œè„šæœ¬ï¼‰ï¼ŒåŒæ—¶ç»“åˆä½ ä»“åº“é‡Œç°æœ‰çš„ workflows åšå¢é‡æ”¹é€ ã€‚

ä¸€ã€GitHub Actions workflow éƒ½èƒ½æ€ä¹ˆè§¦å‘ï¼Ÿ
å¸¸ç”¨è§¦å‘æºï¼ˆon:ï¼‰åˆ†å‡ å¤§ç±»â€”â€”ä¸‹é¢æ¯ç‚¹åéƒ½é™„ä¸Šå®˜æ–¹è¦ç‚¹ï¼š

æºä»£ç äº‹ä»¶

pushã€pull_requestã€pull_request_targetã€merge_groupï¼ˆé…åˆ Merge Queueï¼‰ã€‚

merge_group ä¸“é—¨ç»™åˆå¹¶é˜Ÿåˆ—ç”¨ï¼Œè¦æ±‚æŠŠå®ƒä¹Ÿå†™è¿› CIï¼Œå¦åˆ™è¿›é˜Ÿåˆ—åä¸ä¼šè·‘å¿…é¡»æ£€æŸ¥ã€‚
GitHub Docs

pull_request_target åœ¨åŸºä»“ä¸Šä¸‹æ–‡é‡Œè·‘ã€å¯ç”¨æœºå¯†/å†™æƒé™ï¼Œä½†è¦ä¸¥æ ¼é¿å…ç­¾å‡ºå¹¶æ‰§è¡Œæ¥ PR çš„ä¸å¯ä¿¡ä»£ç ï¼ˆå¦åˆ™â€œpwn requestâ€é£é™©ï¼Œé€šå¸¸ç”¨å®ƒåšè¯„è®º/æ‰“æ ‡ç­¾è¿™ç±»åªè¯»åˆ†æï¼‰ã€‚
GitHub Docs
+2
GitHub Security Lab
+2

åä½œäº’åŠ¨äº‹ä»¶

issuesã€issue_commentã€pull_request_reviewã€pull_request_review_comment ç­‰ï¼ˆå¯ç”¨ types: ç²¾ç¡®åˆ° opened/edited/submitted ç­‰ï¼‰ã€‚
GitHub Docs
+1

æ‰‹åŠ¨&å¤–éƒ¨è§¦å‘

workflow_dispatchï¼ˆæ‰‹ç‚¹æˆ– API/CLI è§¦å‘ï¼Œå¯å¸¦ inputsï¼‰

repository_dispatchï¼ˆå¤–éƒ¨ç³»ç»Ÿ/è„šæœ¬å‘äº‹ä»¶åˆ°ä»“åº“ï¼Œæºå¸¦è‡ªå®šä¹‰ payloadï¼‰ã€‚
GitHub Docs

å·¥ä½œæµçº§è”/å¤ç”¨

workflow_runï¼ˆå½“æŸä¸ª workflow å®Œæˆ/è¯·æ±‚ æ—¶å†è§¦å‘å¦ä¸€ä¸ªï¼‰

workflow_callï¼ˆå¯å¤ç”¨å·¥ä½œæµï¼Œè¢«â€œè°ƒç”¨â€åƒå‡½æ•°ï¼Œæ”¯æŒ inputs/secretsï¼‰ã€‚
GitHub
+1

å®šæ—¶ä¸å…¶ä»–

scheduleï¼ˆcronï¼‰ã€releaseã€registry_packageã€page_build ç­‰ã€‚
GitHub Docs

å…³é”®è§„åˆ™ï¼šç”¨ GITHUB_TOKEN åœ¨å·¥ä½œæµ A é‡Œäº§ç”Ÿçš„äº‹ä»¶ï¼ˆæ¯”å¦‚ push/labelï¼‰ï¼Œé»˜è®¤ä¸ä¼šè§¦å‘å…¶å®ƒå·¥ä½œæµï¼ˆé¿å…é€’å½’ç‚¸åº“ï¼‰ï¼›è‹¥ä½ ç¡®å®è¦â€œå·¥ä½œæµè§¦å‘å·¥ä½œæµâ€ï¼Œéœ€æ”¹ç”¨ PAT æˆ– GitHub App tokenã€‚
GitHub Docs

äºŒã€workflow ä¹‹é—´èƒ½å½“â€œé’©å­â€äº’ç›¸è§¦å‘å—ï¼Ÿ
å¯ä»¥ï¼Œä¸»æµæœ‰ä¸‰æ‹›ï¼š

workflow_runï¼šA è·‘å®Œï¼ˆtypes: [completed]ï¼‰ï¼Œè‡ªåŠ¨æ‹‰èµ· Bï¼›B é‡Œå¯æ ¹æ® github.event.workflow_run.conclusion åšåˆ†æ”¯ï¼ˆä»…åœ¨ A æˆåŠŸæ‰ç»§ç»­ï¼‰ã€‚
GitHub

workflow_callï¼šæŠŠé€šç”¨é˜¶æ®µï¼ˆå¦‚æ„å»º+æµ‹è¯•ï¼‰åšæˆâ€œå¯å¤ç”¨ workflowâ€ï¼Œç”±ä¸åŒå·¥ä½œæµé€šè¿‡ jobs.<id>.uses: ./.github/workflows/reusable.yml ç›´æ¥è°ƒç”¨ï¼Œè§„èŒƒã€å¯ä¼ å‚/æœºå¯†ã€‚
GitHub Docs

repository_dispatch/workflow_dispatchï¼šå¤–éƒ¨è„šæœ¬æˆ– botï¼ˆæ¯”å¦‚ Codex/Claudeï¼‰åœ¨åˆé€‚æ—¶æœºä¸»åŠ¨è§¦å‘ã€‚
GitHub Docs

å®‰å…¨æé†’ï¼šå…¬å…±ä»“åº“å¯¹ pull_request_target è¦æ ¼å¤–å°å¿ƒï¼Œä¸è¦åœ¨å®ƒé‡Œå¤´æ‰§è¡Œ PR åˆ†æ”¯çš„è„šæœ¬ï¼ˆnpm installã€docker build ç­‰ï¼‰ï¼Œåªåšè¯»/è¯„è®ºç±»åŠ¨ä½œå³å¯ã€‚
GitHub Security Lab

ä¸‰ã€ç»“åˆä½ ä»“åº“ç°çŠ¶ï¼Œè½åœ°ä¸€æ¡ã€Œæœ¬åœ°æµ‹è¿‡â†’è‡ªåŠ¨å¼€ PRâ†’AI è¯„å®¡â†’æµ‹è¯•/éƒ¨ç½²â†’å¾ªç¯ä¿®å¤ã€æµæ°´çº¿
ä½ çš„ storyapp ä»“åº“å·²ç»æœ‰è¿™äº›å·¥ä½œæµï¼š

CIï¼ˆæ„å»º+å•æµ‹+E2Eï¼‰ï¼šè§¦å‘ push/pull_request/workflow_dispatchï¼›å« Fast checks ä¸ Deep testsï¼ˆPlaywright + docker composeï¼‰ ä¸¤é˜¶æ®µã€‚
ci

 
ci

 
ci


AI è¯„å®¡ï¼ˆClaude Codeï¼‰ï¼šcodex-review.yml & pr-auto-review.yml éƒ½ä¼šåœ¨ PR æ‰“å¼€/åŒæ­¥æ—¶è·‘å¹¶å›å¸–ã€‚å»ºè®®ä¿ç•™ä¸€ä¸ªï¼Œé¿å…åŒæœºå™¨äººé‡å¤è¯„è®ºã€‚
codex-review

 
pr-auto-review


@claude æåŠå“åº”ï¼šç›‘å¬ issue_comment çš„ @claudeï¼Œç”¨æ¥å¯¹è¯/è¾…åŠ©ã€‚
claude-mentions

 
claude-mentions


åŸºäºæ­¤ï¼Œæˆ‘ç»™ä½ æœ€å°å¢é‡çš„å®ç°æ–¹æ¡ˆï¼ˆå«è„šæœ¬+YAMLï¼‰ï¼Œä¸‰æ­¥èµ°ï¼š

Step 0ï¼ˆæœ¬åœ°ï¼‰ï¼šå®¹å™¨åŒ–è‡ªæ£€é€šè¿‡ â†’ è‡ªåŠ¨å¼€ PR
WSL å‹å¥½è„šæœ¬ï¼ˆæ”¾åˆ° scripts/prepr.shï¼‰ï¼Œæœ¬åœ°å…ˆèµ· CI åŒæ¬¾çš„ composeï¼Œè·‘ä¸€è½® E2Eï¼Œå†ç”¨ gh å¼€ PRã€‚PR æè¿°å¯è®© Codex CLI ç”Ÿæˆï¼ˆéäº¤äº’ exec æ¨¡å¼ï¼‰ã€‚

#!/usr/bin/env bash
set -euo pipefail

BR=${1:-"feature/auto-pr-$(date +%Y%m%d-%H%M)"}
BASE=${2:-"master"}   # æˆ– main
TITLE=${3:-"chore: æœ¬åœ°å®¹å™¨è‡ªæ£€é€šè¿‡ï¼Œæäº¤ PR è¿›å…¥ AI è¯„å®¡ä¸é›†æˆæµ‹è¯•"}

# 1) æœ¬åœ° E2Eï¼šä¸ CI åŒæ­¥
docker compose -f docker-compose.ci.yml up -d --build
trap 'docker compose -f docker-compose.ci.yml down -v' EXIT
timeout 120 bash -c 'until docker compose -f docker-compose.ci.yml ps | grep -E "(mongo|app).*healthy"; do sleep 5; done'
npm ci
npx playwright install --with-deps
npm test

# 2) ç”Ÿæˆ PR æè¿°ï¼ˆç”¨ Codexï¼‰
# éœ€è¦å…ˆ npm i -g @openai/codex && å·²ç™»å½•æˆ–è®¾ç½® OPENAI_API_KEY
codex exec 'ä»æœ€è¿‘ä¸€æ¬¡æäº¤èµ·ï¼Œæ‰«ææœ¬åˆ†æ”¯å˜æ›´ä¸æµ‹è¯•æ—¥å¿—ï¼Œç”Ÿæˆä¸­æ–‡ PR æè¿°ï¼ˆå«å˜æ›´åˆ—è¡¨/é£é™©/æµ‹è¯•è¦ç‚¹ï¼‰ï¼Œè¾“å‡ºåˆ° pr-body.md'

# 3) åˆ›å»ºåˆ†æ”¯ & æ¨é€ & å¼€ PR
git checkout -b "$BR"
git push -u origin "$BR"
gh pr create -B "$BASE" -H "$BR" -t "$TITLE" -F pr-body.md
Codex CLI çš„ exec éäº¤äº’è‡ªåŠ¨æ¨¡å¼å·²å®˜æ–¹æ”¯æŒï¼ŒWindows å»ºè®®åœ¨ WSL ä¸‹è¿è¡Œï¼›å¯ç”¨ --model æˆ– /model æŒ‡ä»¤é€‰æ¨¡å‹ã€‚
developers.openai.com
+1

Step 1ï¼ˆäº‘ç«¯ï¼‰ï¼šPR æ‰“å¼€ â†’ CI + AI è¯„å®¡
ä½ ç°æœ‰ CI å·²è¦†ç›– pull_requestï¼Œä¸” E2E é˜¶æ®µä»…åœ¨éè‰ç¨¿ PR è·‘ï¼Œéå¸¸åˆç†ã€‚
ci


AI è¯„å®¡å·²é…å¥½ Anthropic Claude Code Actionï¼Œå®˜æ–¹æ•™ç¨‹ä¸ action ä»“åº“éƒ½é½å…¨ã€‚è‹¥è¦ç»Ÿä¸€ï¼šä¿ç•™ pr-auto-review.ymlï¼Œå…³é—­ codex-review.ymlï¼Œæˆ–åä¹‹ï¼ˆé¿å…åŒè¯„å®¡ï¼‰ã€‚
Anthropic
+1

Step 2ï¼ˆäº‘ç«¯ï¼‰ï¼šCI æˆåŠŸ â†’ è‡ªåŠ¨éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒï¼ˆstagingï¼‰
æ–°å¢ä¸€ä¸ª deploy-staging-on-ci-success.ymlï¼Œç”¨ workflow_run ç›‘å¬ CI å·¥ä½œæµå®Œæˆï¼›åªåœ¨ CI æˆåŠŸ + äº‹ä»¶æ¥è‡ª pull_request æ—¶éƒ¨ç½²ã€‚éƒ¨ç½²å†…å®¹æŒ‰ä½ çš„å®é™…æ–¹å¼ï¼ˆK8s/SSH/å®¹å™¨æœåŠ¡/GHCRï¼‰æ›¿æ¢ç¤ºä¾‹æ­¥éª¤ã€‚

name: Deploy to Staging on CI success

on:
  workflow_run:
    workflows: ["CI"]            # å¯¹åº”ä½  ci.yml çš„ name: CI
    types: [completed]

jobs:
  deploy:
    if: >
      ${{
        github.event.workflow_run.conclusion == 'success' &&
        github.event.workflow_run.event == 'pull_request'
      }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
      deployments: write
    environment:
      name: staging
      url: ${{ steps.deploy.outputs.url }}

    steps:
      - name: Checkout PR commit
        uses: actions/checkout@v4
        with:
          # ç­¾å‡ºè§¦å‘ CI çš„é‚£æ¬¡æäº¤
          ref: ${{ github.event.workflow_run.head_sha }}

      # å¦‚éœ€è·¨å·¥ä½œæµå–æ„å»ºäº§ç‰©ï¼Œå¯ä¸‹è½½æŒ‡å®š run çš„ artifactï¼ˆv4 æ”¯æŒ run-idï¼‰
      - name: Download build artifacts (optional)
        uses: actions/download-artifact@v4
        with:
          run-id: ${{ github.event.workflow_run.id }}
          # name/pattern æŒ‰ä½ çš„ CI ä¸Šä¼ åç§°è°ƒæ•´

      - name: Deploy to staging
        id: deploy
        run: |
          # TODO: æ›¿æ¢æˆä½ çš„éƒ¨ç½²å‘½ä»¤ï¼ˆç¤ºä¾‹ï¼šSSH/Helm/compose up/æ¨é•œåƒç­‰ï¼‰
          echo "url=https://staging.example.com/${{ github.event.workflow_run.head_branch }}" >> $GITHUB_OUTPUT
workflow_run çš„ç»“è®º/åˆ†æ”¯ä¿¡æ¯å¯ä» github.event.workflow_run.* è¯»å–ï¼Œå¸¸è§åšæ³•æ˜¯åªåœ¨ conclusion == 'success' æ—¶ç»§ç»­ã€‚
GitHub

Step 3ï¼ˆäº‘ç«¯ï¼‰ï¼šè¯„è®ºè§¦å‘è‡ªåŠ¨ä¿®å¤ï¼ˆCodexï¼‰ï¼Œæ¨å› PR è§¦å‘é‡æµ‹
ç»™å‡ºä¸€ä¸ªâ€œè¯„è®ºå³ä¿®å¤â€çš„æœºå™¨äººå·¥ä½œæµï¼šå½“ç»´æŠ¤è€…è¯„è®º /codex fix æ—¶ï¼ŒCodex åœ¨ PR åˆ†æ”¯ä¸Šå°è¯•ä¿®å¤å¹¶ pushï¼›push å°†è§¦å‘ä½ çš„ CI é‡æ–°æµ‹è¯•ã€‚

ç”±äº GITHUB_TOKEN çš„äº‹ä»¶ä¸ä¼šå†è§¦å‘å…¶ä»–å·¥ä½œæµï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ PAT æˆ– GitHub App tokenï¼ˆå¦‚ CI_PATï¼‰è¿›è¡Œ pushï¼Œä»¥ä¿è¯åç»­ CI ä¼šè¢«è§¦å‘ã€‚
GitHub Docs

name: Codex Autofix on Comment

on:
  issue_comment:
    types: [created]

jobs:
  codex-autofix:
    # ä»…åœ¨ PR ä¸‹çš„è¯„è®ºï¼Œä¸”è¯„è®ºè€…ä¸ºæˆå‘˜/æ‰€æœ‰è€…ï¼Œä¸”åŒ…å«æŒ‡ä»¤æ—¶è¿è¡Œ
    if: >
      ${{ github.event.issue.pull_request &&
          contains('OWNER,MEMBER,COLLABORATOR', github.event.comment.author_association) &&
          contains(github.event.comment.body, '/codex fix') }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout PR HEAD (unsafe code must NOT use secrets)
        uses: actions/checkout@v4
        with:
          # ä»¥å¼•ç”¨æ–¹å¼ç­¾å‡º PR headï¼ˆæ— éœ€é¢å¤– APIï¼‰
          ref: refs/pull/${{ github.event.issue.number }}/head

      - name: Setup Node & Codex CLI
        uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm i -g @openai/codex

      - name: Let Codex attempt an autofix and push
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GH_TOKEN: ${{ secrets.CI_PAT }}   # PAT/APP tokenï¼Œç¡®ä¿ push èƒ½è§¦å‘åç»­å·¥ä½œæµ
        run: |
          # éäº¤äº’è‡ªåŠ¨æ¨¡å¼ï¼šè·‘æµ‹â†’ä¿®â†’æäº¤â†’æ¨é€
          codex exec --full-auto "
            å…ˆè¿è¡Œï¼šnpm ci && npm run build --if-present && npm testï¼›
            è‹¥å¤±è´¥ï¼šå®šä½å¤±è´¥æ ¹å› ï¼Œä¿®æ”¹ä»£ç ä¸æµ‹è¯•ï¼Œæäº¤ä¿¡æ¯ä¸º 'codex: autofix'ï¼›
            å†æ¬¡è¿è¡Œæµ‹è¯•ç›´è‡³é€šè¿‡æˆ–ç»™å‡ºå¤±è´¥åŸå› ï¼›æœ€å push åˆ°å½“å‰åˆ†æ”¯ã€‚"
Step 4ï¼ˆå¯é€‰ï¼‰ï¼šæ¥å…¥ Merge Queue
è‹¥ä½ å‡†å¤‡å¯ç”¨ GitHub Merge Queueï¼ŒæŠŠ CI é‡Œçš„è§¦å‘åŠ ä¸Š merge_groupï¼Œä»¥ä¾¿ PR å…¥é˜ŸååŒæ ·è·‘å¿…é¡»æ£€æŸ¥ï¼š

on:
  pull_request:
  merge_group:
å¦åˆ™å…¥é˜Ÿä¼šå› ç¼ºå°‘æ£€æŸ¥è€Œæ— æ³•æ¨è¿›ã€‚
GitHub Docs

Step 5ï¼ˆå¯é€‰ï¼‰ï¼šå¤ç”¨å·¥ä½œæµä¸å®‰å…¨åŠ å›º
æŠŠâ€œæ„å»º+å•æµ‹+E2Eâ€åšæˆ workflow_call çš„å¤ç”¨å·¥ä½œæµï¼ˆæ¯”å¦‚ reusable-tests.ymlï¼‰ï¼ŒPR/åˆå¹¶é˜Ÿåˆ—/å‘å¸ƒæµç¨‹éƒ½ç”¨åŒä¸€ä¸ªâ€œæºâ€ã€‚
GitHub Docs

å…¬å…±ä»“åº“é‡Œéœ€è¦é¿å…åœ¨ pull_request_target ä¸Šæ‰§è¡Œä¸å¯ä¿¡ä»£ç ï¼›è‹¥å¿…é¡»ç”¨å®ƒï¼ˆä¾‹å¦‚æƒ³å®‰å…¨å†™å…¥è¯„è®º/æ ‡ç­¾å¹¶ç”¨åˆ°æœºå¯†ï¼‰ï¼Œåªåšâ€œè¯» diff / å›å¸–â€ï¼Œä¸è¦ npm install/docker build/æ‰§è¡Œè„šæœ¬ã€‚è¯¦è§å®˜æ–¹å®‰å…¨æŒ‡å—ä¸å®è·µã€‚
GitHub Security Lab

å››ã€æŠŠ Claude Code ä¸ Codex èå…¥æµæ°´çº¿çš„å…·ä½“å»ºè®®
Claude Codeï¼ˆGitHub Actionï¼‰ï¼šä½ å·²ç»åœ¨ç”¨ anthropics/claude-code-action@v1ï¼Œå®ƒæ”¯æŒåœ¨ PR åœºæ™¯åšç»“æ„åŒ–è¯„å®¡ã€è·Ÿè¿›è¿›åº¦ã€å“åº” @claudeï¼ŒæŒ‰ç…§å®˜æ–¹æŒ‡å¼•è®¾ç½® ANTHROPIC_API_KEY/BASE_URL å³å¯ã€‚
Anthropic
+1

Codexï¼ˆCLI/äº‘ç«¯ï¼‰ï¼š

æœ¬åœ°ï¼šç”¨ codex exec ç”Ÿæˆ PR æè¿°ã€è¾…åŠ©ä¿®å¤ï¼›Windows å»ºè®®åœ¨ WSL ä¸‹ä½¿ç”¨ã€‚
developers.openai.com

äº‘ç«¯ï¼šåƒä¸Šé¢â€œè¯„è®ºè‡ªåŠ¨ä¿®å¤â€é‚£æ ·åœ¨ Action ä¸­å®‰è£… @openai/codex å¹¶æ‰§è¡Œ codex exec --full-autoï¼Œç”¨ PAT è¿›è¡Œ push è§¦å‘ CIã€‚

å¦å¤–ï¼ŒCodex è¿‘æœŸæ›´æ–°ä¹ŸåŠ å¼ºäº† PR è‡ªåŠ¨è¯„å®¡ï¼ˆ@codex at PRï¼‰ä¸ IDE/ç»ˆç«¯ä¸€ä½“åŒ–ã€‚å¦‚æœä½ æƒ³å’Œ Claude å¹¶å­˜ï¼Œå¯ç”¨ æ ‡ç­¾ æ§åˆ¶è§¦å‘ï¼ˆå¦‚ AI:codex åªè·‘ Codex è¯„å®¡ï¼‰ã€‚
IT Pro

äº”ã€ä½ ä»“åº“å½“å‰é…ç½®çš„å°ä¼˜åŒ–æ¸…å•
é¿å…åŒè¯„å®¡ï¼šcodex-review.yml ä¸ pr-auto-review.yml ä½œç”¨é‡å ï¼Œå»ºè®®åªç•™ä¸€ä¸ªï¼Œæˆ–ä»¥ path filters/label åˆ†æµï¼ˆå¦åˆ™ PR ä¸‹ä¼šæœ‰ä¸¤æ¡æœºå™¨äººé•¿è¯„ï¼‰ã€‚
codex-review

 
pr-auto-review


CI è§¦å‘å·²è¦†ç›–ï¼šä½ çš„ CI åŒæ—¶ç›‘å¬ push/pull_request/workflow_dispatchï¼Œå¹¶å¸¦å¹¶å‘é˜²æŠ–ï¼ˆconcurrencyï¼‰ï¼Œè¿™ç‚¹å¾ˆå¥½ï¼Œä¿ç•™å³å¯ã€‚
ci


E2E å¯åŠ¨ï¼šdeep-tests ç”¨ docker-compose.ci.yml æ‹‰èµ·æœåŠ¡å¹¶ç­‰å¾… healthyï¼Œå†è£…æµè§ˆå™¨è·‘ Playwrightï¼Œåšæ³•æ ‡å‡†ï¼›å¯è€ƒè™‘æŠŠ compose å¥åº·æ£€æŸ¥æ—¶é™ä» 120s æå‡åˆ° 180sï¼Œå‡å°‘å¶å‘ flakinessã€‚
ci


Q&A é€Ÿç­”
æˆ‘èƒ½åœ¨ä¸€ä¸ªå·¥ä½œæµé‡Œ push è§¦å‘å¦ä¸€ä¸ªå·¥ä½œæµå—ï¼Ÿ
é»˜è®¤ç”¨ GITHUB_TOKEN ä¸ä¼šè§¦å‘ï¼ˆé˜²æ­¢é€’å½’ï¼‰ã€‚è¦è§¦å‘è¯·ç”¨ PAT æˆ– GitHub App tokenã€‚
GitHub Docs

å·¥ä½œæµâ€œåƒé’©å­â€é‚£æ ·ä¸²èµ·æ¥çš„æœ€ä½³å§¿åŠ¿ï¼Ÿ
workflow_runï¼ˆä¸²è¿ Aâ†’Bï¼‰ã€workflow_callï¼ˆå¤ç”¨æ­¥éª¤ï¼‰ã€repository_dispatchï¼ˆå¤–éƒ¨/æœºå™¨äººè§¦å‘ï¼‰ï¼Œä¸‰ä»¶å¥—ã€‚
GitHub
+1

Merge Queue è¦åšä»€ä¹ˆæ”¹åŠ¨ï¼Ÿ
CI é‡ŒåŠ ä¸Š merge_group è§¦å‘ï¼Œå¦åˆ™å…¥é˜Ÿä¸è·‘æ£€æŸ¥ã€‚
GitHub Docs

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥æŠŠä¸Šé¢çš„ YAML å’Œè„šæœ¬ç›´æ¥æŒ‰ä½ ä»“åº“ç›®å½•ç»“æ„æ‰“åŒ…æˆ PRï¼ˆç”¨æˆ‘ä»¬è¿™å¥—â€œStep 0â€è„šæœ¬è·‘ä¸€éå°±èƒ½å¼€ PRï¼‰ã€‚å¦å¤–ï¼Œä½ å·²ç»å¼€äº†ä»“åº“è¿æ¥å™¨ï¼Œæˆ‘è¿™è¾¹çœ‹åˆ°çš„æ–‡ä»¶ä½ç½®ä¸è§¦å‘æ¡ä»¶å¦‚ä¸‹ï¼Œä¾›ä½ å¤æ ¸ï¼š

.github/workflows/ci.ymlï¼šè§¦å‘ä¸åˆ†é˜¶æ®µé…ç½®è§ L3â€“L13ã€L18â€“L29ã€L69â€“L76ã€‚
ci

 
ci

 
ci


.github/workflows/codex-review.ymlï¼šPR æ‰“å¼€/åŒæ­¥è§¦å‘ï¼Œå¸¦ use_sticky_commentã€‚
codex-review

 
codex-review


.github/workflows/pr-auto-review.ymlï¼šPR æ‰“å¼€/åŒæ­¥è§¦å‘ï¼Œç»“æ„åŒ–ä¸­æ–‡å®¡æŸ¥æ¨¡æ¿ã€‚
pr-auto-review

 
pr-auto-review


.github/workflows/claude-mentions.ymlï¼šç›‘å¬ issue_comment çš„ @claudeã€‚
claude-mentions

 
claude-mentions


éœ€è¦æˆ‘æŠŠ deploy-staging-on-ci-success.yml å’Œ â€œè¯„è®ºè‡ªåŠ¨ä¿®å¤â€çš„å·¥ä½œæµç›´æ¥ç”Ÿæˆæˆä¸¤ä¸ªæ–‡ä»¶ç»™ä½ å—ï¼Ÿæˆ‘å¯ä»¥æŒ‰ä½ ç°åœ¨çš„ docker-compose.ci.yml å‡è®¾å…ˆæ”¾ä¸€ä¸ª SSH/compose çš„æœ€å°å¯è¿è¡Œç‰ˆæœ¬ã€‚


Sources
You said:
æˆ‘æ˜¯chatgpt PRoï¼Œæˆ‘ä¹Ÿå¸Œæœ›å……åˆ†åˆ©ç”¨codexçš„CIèƒ½åŠ›ï¼Œç°åœ¨codex cliå·²ç»è‡ªå·±åŠ¨è§¦å‘prè¯„å®¡äº†ï¼Œæœ‰æ²¡æœ‰æ›´å¤šèƒ½åŠ›å¯ä»¥ç”¨çš„
ChatGPT said:

Thought for 1m 8s
å½“ç„¶æœ‰ï¼ä½ å·²ç»æŠŠ Codex CLI çš„ PR è¯„å®¡ è·‘èµ·æ¥äº†ï¼ˆå¤ªæ£’ï¼‰ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿˜èƒ½æŠŠ Codex çš„ã€Œäº‘ç«¯ä»»åŠ¡ã€ç¯å¢ƒã€AGENTS.md çº¦å®šã€å’Œ CLI çš„ exec è„šæœ¬åŒ–èƒ½åŠ›ä¸²è¿›æ•´æ¡ CI/CD æµæ°´çº¿ï¼Œåšåˆ°ä»â€œå¤±è´¥å®šä½â†’è¡¥æµ‹â†’è‡ªåŠ¨ä¿®å¤â†’é‡æ–°éªŒæ”¶â†’å‘å¸ƒè¯´æ˜/å›¾è°±/ä½“æ£€â€å…¨é—­ç¯ã€‚ä¸‹é¢ç»™ä½ ä¸€å¥—â€œç›´æ¥å¯ç”¨â€çš„å¢å¼ºæ¸…å• + å¯æ‹·è´ YAML æ¨¡æ¿ã€‚

ä½ è¿˜å¯ä»¥å¯ç”¨çš„ 10 é¡¹ Codex èƒ½åŠ›ï¼ˆCI å‹å¥½ï¼‰
å¤±è´¥æ—¥å¿— â†’ è‡ªåŠ¨ä¿®å¤ï¼ˆAutofix on failureï¼‰
CI ä»»ä¸€ Job å¤±è´¥æ—¶ï¼Œè§¦å‘â€œCodex ä¿®å¤â€å·¥ä½œæµï¼šè¯»å–å¤±è´¥æ—¥å¿—ã€æœ€å°åŒ–å˜æ›´ä¿®å¤ã€è¡¥ä¸Šç¼ºå¤±æµ‹è¯•ã€æ¨å›åŒåˆ†æ”¯ï¼Œå†è§¦å‘ CI é‡è·‘ã€‚CLI å¯ç”¨éäº¤äº’æ¨¡å¼ codex exec "fix the CI failure" å®Œæˆè¿™ä»¶äº‹ã€‚
OpenAI Help Center

æ™ºèƒ½è¡¥æµ‹ï¼ˆTest authoring for touched filesï¼‰
åœ¨ PR æ‰“å¼€/åŒæ­¥æ—¶ç”± Codex è‡ªåŠ¨ç”Ÿæˆ/å®Œå–„å—å½±å“æ–‡ä»¶çš„å•æµ‹/ç«¯æµ‹ï¼Œå†è·‘ä¸€æ¬¡ Playwright/Vitestã€‚å‘½ä»¤åŒæ ·èµ° codex exec éäº¤äº’ã€‚
OpenAI Help Center

PR é£é™©ä¸æ ¡éªŒæ¸…å•ç”Ÿæˆ
PR æ‰“å¼€æ—¶ï¼Œè®© Codex äº§å‡ºâ€œå˜æ›´å½±å“é¢+é£é™©ç‚¹+å›å½’ç‚¹+å›æ»šæ–¹æ¡ˆâ€çš„ç»“æ„åŒ– Markdownï¼Œç›´æ¥å›å¸–æˆ–å†™å…¥ PR_PLAN.md ä½œä¸ºå®¡æŸ¥æ¨¡æ¿ã€‚CLI æ¨¡å¼å³å¯ã€‚
OpenAI Help Center

åœ¨ GitHub å†…ç”¨ @codex review è®© Codex å‚ä¸è¯„å®¡ï¼ˆäº‘ç«¯ï¼‰
åœ¨ ChatGPT çš„ Codex è®¾ç½®é‡Œå¯¹ä»“åº“å¯ç”¨ â€œCode reviewâ€ï¼Œéšååœ¨ PR é‡Œè¯„è®º @codex review å³å¯è§¦å‘äº‘ç«¯å®¹å™¨è·‘å®Œæ•´è¯„å®¡æµç¨‹ï¼Œå¹¶ä»¥å¸¸è§„ Code Review çš„å½¢å¼å›åˆ° PRã€‚
developers.openai.com

å°ç‚¹ç›ï¼šè¿™æ¡ä¸ä¾èµ–ä½ çš„ Runner èµ„æºï¼Œé€‚åˆå¤§ä»“å¹¶å‘å®¡æŸ¥ï¼›å¯ç”¨è·¯å¾„è§ Codex Cloud æ–‡æ¡£ï¼ˆå…ˆåœ¨ chatgpt.com/codex è¿æ¥ GitHubï¼‰ã€‚
developers.openai.com

æŠŠ AGENTS.md çº³å…¥ä»“åº“è§„èŒƒï¼ˆè®©ä»£ç†â€œæŒ‰ä½ è¯´çš„åšâ€ï¼‰
åœ¨ä»“åº“æ ¹ç›®å½•å†™æ˜ã€Œæ€ä¹ˆå®‰è£…ã€æ€ä¹ˆæµ‹ã€Lint/æ ¼å¼åŒ–è§„åˆ™ã€åˆå¹¶å‰å¿…è·‘å‘½ä»¤ã€ã€‚å¤šæ•°ä»£ç†ï¼ˆå« Codexï¼‰ä¼šä¼˜å…ˆè¯»å–å¹¶è‡ªåŠ¨æ‰§è¡Œä½ åˆ—å‡ºçš„æµ‹è¯•/æ ¡éªŒå‘½ä»¤ï¼Œå¤§å¹…é™ä½èµ°åé£é™©ã€‚æ”¯æŒåˆ†å±‚ï¼ˆå­åŒ…æ”¾è‡ªå·±çš„ AGENTS.mdï¼Œè¿‘è€…ä¼˜å…ˆï¼‰ã€‚
agents.md

Codex Cloud è‡ªå®šä¹‰ç¯å¢ƒ & ä¾èµ–é¢„çƒ­
ä¸ºäº‘ç«¯ä»»åŠ¡é…ç½®â€œç¯å¢ƒâ€ï¼šè®¾ç½® Node/Python/Go ç‰ˆæœ¬ã€é¢„è£…ä¾èµ–/å·¥å…·ã€ç»´æŠ¤è„šæœ¬ã€ç¼“å­˜ç­–ç•¥ï¼›é»˜è®¤ codex-universal åŸºç¡€é•œåƒå·²å¸¦å¸¸è§è¯­è¨€ä¸åŒ…ï¼Œå¯ç”¨ç¯å¢ƒå˜é‡å®šç‰ˆæœ¬ï¼ˆå¦‚ CODEX_ENV_NODE_VERSION=20/22ï¼‰ã€‚
developers.openai.com
+1

å®‰å…¨ä¸Šç½‘ç™½åå•
äº‘ç«¯ä»»åŠ¡é»˜è®¤ç¦ç½‘ï¼›ç¡®éœ€è”ç½‘ï¼ˆå¦‚è£…ä¾èµ–/æ‹‰ç§æœï¼‰ï¼Œå¯ä»¥åœ¨ç¯å¢ƒé‡Œä¸¥æ ¼ç™½åå•åŸŸå+HTTP æ–¹æ³•ï¼ˆä»… GET/HEAD/OPTIONSï¼‰ï¼Œé™ä½æ³¨å…¥/å¤–ä¼ é£é™©ã€‚
developers.openai.com

å¤±è´¥ Job çš„â€œäºŒæ¬¡æ„è§â€
å³ä¾¿ä¸è®©å®ƒæ”¹ä»£ç ï¼Œä¹Ÿå¯ç”¨ Codex è¯»å–æ„å»º/æµ‹è¯•æ—¥å¿—ç»™å‡ºâ€œå®šä½æŠ¥å‘Š+æœ€å°å¤ç°è„šæœ¬+ä¿®å¤è‰æ¡ˆâ€ï¼Œç”±äººç±»ç¡®è®¤åæ‰‹æ¨ä¿®å¤ã€‚
OpenAI Help Center

å‘å¸ƒè¯´æ˜ / å˜æ›´æ—¥å¿— / æ¶æ„å›¾è‡ªåŠ¨äº§å‡º
åœ¨åˆå¹¶æˆ–æ‰“ tag æ—¶ï¼Œè®© Codex æ±‡æ€» conventional commits / PR æè¿°ï¼Œç”Ÿæˆ CHANGELOG.md ä¸â€œç«¯åˆ°ç«¯æ—¶åº/ä¾èµ–å…³ç³»çš„ Mermaid å›¾â€å¹¶ä¸Šä¼ ä¸ºæ„å»ºäº§ç‰©ã€‚
OpenAI Help Center

PR å†…ä¸€é”®è½¬äº‘ç«¯ä»»åŠ¡
PR é‡Œ @codex <ä»»æ„ä»»åŠ¡>ï¼ˆé reviewï¼‰ä¼šä»¥ PR ä¸Šä¸‹æ–‡å¼€ä¸€ä¸ªäº‘ç«¯ Taskï¼ˆèƒŒæ™¯æ‰§è¡Œã€å¯å¹¶è¡Œï¼‰ï¼Œä¾‹å¦‚â€œä¸ºæœ¬ PR ç”Ÿæˆ E2E è¯•éªŒåœºæ™¯å¹¶ç»™å‡ºè¦†ç›–ç‡å·®è·â€ã€‚
developers.openai.com

è´´å£«ï¼šä½ æ˜¯ ChatGPT Proï¼Œå¯ç›´æ¥ç”¨â€œSign in with ChatGPTâ€ç™»å½• Codex CLI/IDEï¼Œå®˜æ–¹å»ºè®®åœ¨ macOS/Linux ä½¿ç”¨ï¼›Windows ç›®å‰å»ºè®®åœ¨ WSL è·‘ã€‚
OpenAI Help Center
+1

ç›´æ¥å¯ç”¨çš„ GitHub Actions æ¨¡æ¿ï¼ˆæ‹·èµ°å³ç”¨ï¼‰
å®‰å…¨è¦ç‚¹ï¼šç”¨ GITHUB_TOKEN å‘èµ·çš„ push ä¸ä¼šè§¦å‘ä¸‹æ¸¸å·¥ä½œæµï¼›å¦‚éœ€â€œæ”¹å®Œâ†’æ¨å›â†’è‡ªåŠ¨é‡æµ‹â€ï¼Œè¯·ç”¨ PAT/GitHub App å†™æƒé™ Tokenï¼ˆä¾‹å¦‚ secrets.CI_PATï¼‰ã€‚
PR å®‰å…¨ï¼šåœ¨å…¬å…±ä»“åº“é¿å…åœ¨ pull_request_target ä¸Šæ‰§è¡Œä¸å¯ä¿¡åˆ†æ”¯ä»£ç ï¼ˆåªåšè¯»/è¯„è®ºï¼‰ã€‚

A. CI å¤±è´¥è‡ªåŠ¨ä¿®å¤ï¼ˆworkflow_run ç›‘å¬ CIï¼‰
name: Codex Autofix when CI fails
on:
  workflow_run:
    workflows: ["CI"]   # è¿™é‡Œå¡«ä½ çš„ä¸» CI workflow åç§°
    types: [completed]

jobs:
  autofix:
    if: ${{ github.event.workflow_run.conclusion == 'failure' && github.event.workflow_run.event == 'pull_request' }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Checkout failing commit
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha }}

      - name: Setup Node & Codex CLI
        uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm i -g @openai/codex

      - name: Autofix with Codex and push
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}  # æˆ–èµ° Sign in æµæ°´çº¿ç”Ÿæˆçš„å‡­æ®
          GH_TOKEN: ${{ secrets.CI_PAT }}                # å¿…é¡»ç”¨ PAT ä»¥ä¾¿ push è§¦å‘ CI
        run: |
          # å–‚ç»™ Codex ä½ ä»“åº“çš„â€œæ€æ ·è·‘æµ‹â€è§„åˆ™ï¼ˆè§ä¸‹æ–¹ AGENTS.mdï¼‰
          echo "çœ‹ ./.github/workflowsã€AGENTS.mdã€package.json çš„ scriptsï¼Œå…ˆè·‘æµ‹è¯•å†ä¿®å¤ã€‚" > codex-plan.md
          codex exec "
            è¯»å– codex-plan.md å’Œæœ€è¿‘ä¸€æ¬¡ CI å¤±è´¥æ—¥å¿—ï¼ˆåœ¨ artifacts æˆ– ./test-resultsï¼‰ï¼›
            å…ˆæ‰§è¡Œï¼špnpm i || npm ciï¼›å†æ‰§è¡Œï¼šnpm run lint --if-present && npm testï¼›
            è‹¥å¤±è´¥ï¼šå®šä½æ ¹å› ï¼Œæœ€å°åŒ–ä¿®æ”¹å¹¶è¡¥è¶³å¿…è¦æµ‹è¯•ï¼Œæäº¤ä¿¡æ¯ 'codex: autofix'ï¼›
            é€šè¿‡å push å›å½“å‰åˆ†æ”¯ã€‚"
B. PR æ™ºèƒ½è¡¥æµ‹ï¼ˆchanged files â†’ ç”Ÿæˆ/å®Œå–„æµ‹è¯•ï¼‰
name: Codex Write/Improve Tests for PR
on:
  pull_request:
    types: [opened, synchronize, ready_for_review]

jobs:
  write-tests:
    if: ${{ !github.event.pull_request.draft }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm i -g @openai/codex

      - name: Generate/Improve tests with Codex
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GH_TOKEN: ${{ secrets.CI_PAT }}
        run: |
          codex exec "
            åŸºäº diffï¼ˆgit diff origin/${{ github.base_ref }}...HEADï¼‰åˆ—å‡ºå—å½±å“æºæ–‡ä»¶ï¼›
            å¯¹æ¯ä¸ªæºæ–‡ä»¶ç”Ÿæˆ/å®Œå–„å¯¹åº”æµ‹è¯•ï¼ˆVitest/Playwrightï¼‰ï¼Œä¿æŒç°æœ‰æµ‹è¯•é£æ ¼ï¼›
            æœ¬åœ°æ‰§è¡Œ npm test é€šè¿‡åï¼Œæäº¤ 'codex: add/upgrade tests' å¹¶ pushã€‚"
C. PR é£é™©ä¸å›å½’ç‚¹æ¸…å•ï¼ˆè¯„è®º/æ–‡ä»¶åŒé€šé“ï¼‰
name: Codex Risk & Test Plan
on:
  pull_request:
    types: [opened, synchronize]

jobs:
  risk-plan:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm i -g @openai/codex

      - name: Generate plan
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          codex exec "
            è¯»å– PR diff ä¸ package.json scriptsï¼›
            äº§å‡ºï¼šå˜æ›´æ‘˜è¦ã€å½±å“é¢ã€é£é™©ç‚¹ã€å›å½’èŒƒå›´ã€å›æ»šæ–¹æ¡ˆã€éœ€æ–°å¢ç”¨ä¾‹åˆ—è¡¨ï¼›
            å°†ç»“æœä¿å­˜åˆ° PR_PLAN.md å¹¶ä½œä¸ºè¯„è®ºè´´åˆ° PRã€‚"
D. åœ¨ PR é‡Œâ€œ@codex reviewâ€ï¼ˆäº‘ç«¯å¯ç”¨æ³•ï¼‰
å…ˆåœ¨ chatgpt.com/codex ç»‘å®š GitHub å¹¶åœ¨ Codex è®¾ç½®ä¸­ä¸ºä»“åº“å¼€å¯ Code reviewã€‚

ä¹‹ååœ¨ PR è¯„è®ºé‡Œå‘ï¼š@codex reviewã€‚Codex ä¼šåœ¨äº‘ç«¯å®¹å™¨æ‹‰ä»£ç ã€è·‘ä½ é…ç½®çš„æµ‹è¯•/æ ¡éªŒå¹¶ä»¥å¸¸è§„ä»£ç è¯„å®¡å½¢å¼å›åˆ° PRã€‚
developers.openai.com
+1

å»ºè®®æŠŠä¸‹é¢çš„ AGENTS.md æ”¾è¿›ä»“åº“æ ¹ç›®å½•ï¼ˆå¹¶æŒ‰å­åŒ…åšåˆ†å±‚ï¼‰
æœ‰äº†å®ƒï¼ŒCodexï¼ˆå«äº‘ç«¯ä»»åŠ¡ï¼‰ä¼šéµå¾ªä½ çš„å®‰è£…/æµ‹è¯•/è§„èŒƒå¹¶è‡ªåŠ¨æ‰§è¡Œå…³é”®å‘½ä»¤ã€‚
agents.md

# AGENTS.mdï¼ˆstoryapp ç¤ºä¾‹ï¼‰

## å®‰è£…ä¸å¯åŠ¨
- Node: 20ï¼ˆæˆ– 22ï¼‰
- å®‰è£…ï¼š`pnpm i`ï¼ˆæ—  pnpm åˆ™ç”¨ `npm ci`ï¼‰
- Devï¼š`pnpm dev`
- æ„å»ºï¼š`pnpm build`
- E2Eï¼š`pnpm playwright install --with-deps && pnpm e2e`
- å•æµ‹ï¼š`pnpm test`
- Lintï¼š`pnpm lint --max-warnings=0 && pnpm format:check`

## CI è¦æ±‚
- PR å¿…é¡»é€šè¿‡ï¼šLintã€å•æµ‹ã€E2Eï¼ˆdocker-compose.ci.ymlï¼‰
- ä¿®æ”¹ä»¥ä¸‹ç›®å½•æ—¶éœ€è¦æ–°å¢/æ›´æ–°æµ‹è¯•ï¼š
  `apps/web/src/**`, `packages/api/src/**`

## ä»£ç é£æ ¼
- TypeScript ä¸¥æ ¼æ¨¡å¼ï¼›å•å¼•å·ã€æ— åˆ†å·ï¼›æäº¤å‰å¿…é¡» `pnpm lint && pnpm test`

## é¢å¤–æ³¨æ„
- Playwright ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼›CI ä¸‹ `CI=true`
- Docker Compose å¥åº·æ£€æŸ¥æœ€é•¿ 180sï¼Œé‡è¯•é—´éš” 5s
ç”¨ Codex Cloud ç¯å¢ƒ æé€Ÿä¸é™é™©ï¼ˆå¯é€‰ä½†å¾ˆé¦™ï¼‰
åœ¨ Codex è®¾ç½®é‡Œä¸ºä»“åº“é…ç½®ç¯å¢ƒï¼š

æŒ‡å®šè¯­è¨€ç‰ˆæœ¬ï¼ˆå¦‚ CODEX_ENV_NODE_VERSION=20ï¼‰ï¼Œé¢„è£…å·¥å…·ï¼ˆVitest/Playwright/ESLintï¼‰ï¼Œå†™å…¥ Setup/Maintenance è„šæœ¬ä¸ç¼“å­˜ç­–ç•¥ã€‚
developers.openai.com
+1

é»˜è®¤ codex-universal é•œåƒå·²å¸¦å¤šè¯­è¨€/å·¥å…·ï¼Œæ”¯æŒ 12 å°æ—¶ç¼“å­˜å¤ç”¨ï¼›å˜æ›´ Setup/å˜é‡/å¯†é’¥ä¼šè‡ªåŠ¨å¤±æ•ˆç¼“å­˜ã€‚
developers.openai.com

è”ç½‘ç­–ç•¥ï¼šä»…åœ¨ Setup é˜¶æ®µå…¨ç½‘å¼€ï¼›Agent é˜¶æ®µé»˜è®¤æ–­ç½‘ï¼Œå¦‚éœ€è®¿é—®å†…ç½‘/ç§æœ‰æºï¼Œè¯·ç²¾ç¡®é…ç½®åŸŸåç™½åå•å’Œå…è®¸çš„ HTTP æ–¹æ³•ï¼ˆå¸¸é… GET/HEAD/OPTIONSï¼‰ã€‚
developers.openai.com

Windows/æœ¬åœ°æœ€ä½³å®è·µï¼ˆé…åˆä½ ç°åœ¨çš„æµç¨‹ï¼‰
Windows è¯·ç”¨ WSL è·‘ Codex CLIï¼ˆå®˜æ–¹å»ºè®®ï¼‰ï¼Œå‡å°‘è·¯å¾„/æƒé™/ä¾èµ–å·®å¼‚ï¼›codex exec ä¸ä½ ç°æœ‰çš„ WSL+Docker+Playwright ä¸€è‡´ã€‚
OpenAI Help Center

ä½ æ˜¯ ChatGPT Proï¼Œå¯ç›´æ¥ Sign in with ChatGPT ç™»å½• Codexï¼ˆå… API Key æŠ˜è…¾ï¼Œé¢åº¦ä¸è®¢é˜…ç»‘å®šï¼‰ã€‚
OpenAI Help Center
+1

å°ç»“ï¼šæ€ä¹ˆæŠŠå®ƒä»¬æ‹¼åˆ°ä½ çš„ç°æœ‰çº¿
ä¿ç•™ä½ ç°æœ‰çš„ CIï¼ˆæ„å»º/å•æµ‹/E2Eã€å¹¶å‘é˜²æŠ–ï¼‰ï¼Œåœ¨å…¶ä¹‹ä¸Šæ–°å¢ä¸‰ä»¶å¥—ï¼š

workflow_run â†’ Autofixï¼ˆA æ¨¡æ¿ï¼‰

PR è¡¥æµ‹ï¼ˆB æ¨¡æ¿ï¼‰

PR é£é™©è®¡åˆ’ï¼ˆC æ¨¡æ¿ï¼‰

éœ€è¦â€œäº‘ç«¯è¯„å®¡/å¹¶è¡Œè·‘æ´»â€çš„ PRï¼Œç”¨ @codex review è§¦å‘ï¼ˆDï¼‰ã€‚
developers.openai.com

è¡¥ä¸Š AGENTS.mdï¼ˆæ ¹ + å­åŒ…ï¼‰ï¼ŒæŠŠä½ çš„â€œå¦‚ä½•å®‰è£…/å¦‚ä½•æµ‹/åˆå…¥é—¨æ§›â€å†™æ¸…æ¥šï¼ŒCodex ä¼šæŒ‰å®ƒæ¥ã€‚
agents.md

è‹¥è¦â€œæ”¹å®Œå³è§¦å‘åç»­å·¥ä½œæµâ€ï¼Œè¯·ç”¨ PAT/App Token pushï¼Œé¿å… GITHUB_TOKEN çš„é»˜è®¤æŠ‘åˆ¶ã€‚

äº‘ç«¯è·‘æ´»æ—¶ï¼Œå»ºè®®é…ç½® Codex Cloud ç¯å¢ƒï¼ˆè¯­è¨€ç‰ˆæœ¬ã€ä¾èµ–ã€ç¼“å­˜ã€è”ç½‘ç™½åå•ï¼‰ã€‚
developers.openai.com
+1

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥æ ¹æ®ä½  repo çš„çœŸå®ç›®å½•æŠŠä¸Šé¢ A/B/C ä¸‰ä¸ª workflow å’Œä¸€ä¸ª AGENTS.md æœ€å°æ¨¡æ¿ç›´æ¥æŒ‰ä½  Monorepo/å‰åç«¯å­åŒ…è½ç›˜ï¼ˆä¿æŒä¸ docker-compose.ci.yml å¯¹é½ï¼‰ã€‚
è¦ä¸è¦æˆ‘ç›´æ¥ç»™å‡ºè¿™å››ä¸ªæ–‡ä»¶çš„ä»“åº“ç›¸å¯¹è·¯å¾„å’Œæœ€ç»ˆå†…å®¹ï¼Ÿ

You said:
çœ‹æ–‡æ¡£ç»“åˆæˆ‘çš„ä»£ç åº“ç»™æˆ‘æœ€ä½³çš„ä¸‹ä¸€æ­¥å»ºè®®
ChatGPT said:
ä½ å¸Œæœ› Codex åœ¨ CI/CD ä¸­æ‰®æ¼”ä»€ä¹ˆæ ·çš„è§’è‰²æ¥é…åˆä½ çš„ä»£ç åº“ï¼Ÿæ¯”å¦‚ï¼š

æ˜¯å¦å¸Œæœ› Codex åœ¨ CI å¤±è´¥æ—¶è‡ªåŠ¨ä¿®å¤å¹¶æ¨é€ï¼Ÿ

æ˜¯å¦å¸Œæœ› PR æ‰“å¼€æ—¶ Codex è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ã€é£é™©æ¸…å•ï¼Ÿ

æ˜¯å¦å¸Œæœ›åˆå¹¶æ—¶ç”± Codex è‡ªåŠ¨äº§å‡ºå‘å¸ƒè¯´æ˜æˆ–æ¶æ„å›¾ï¼Ÿ

ä½ æ˜¯å¦å·²ç»é…ç½®äº† AGENTS.mdï¼ˆæˆ–æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™ä¸€ä¸ªï¼‰ï¼Ÿ

ä½ çš„ä¸»è¦è¯­è¨€å’Œå·¥å…·æ ˆï¼ˆä¾‹å¦‚ Node.js + pnpm + Vitest + Playwrightï¼‰æ˜¯å¦éœ€è¦ç‰¹åˆ«è¯´æ˜æˆ–é€‚é… Codexï¼Ÿ

è¯·æ˜ç¡®ä½ å¸Œæœ›é‡ç‚¹æ¨è¿›å“ªäº›æ–¹å‘ï¼Œæˆ‘ä¼šç»“åˆä½ ä»£ç åº“å’Œ Codex æ–‡æ¡£ç»™å‡ºä¸‹ä¸€æ­¥æœ€ä½³å»ºè®®ã€‚

You said:
1. æ˜¯çš„ï¼Œcodexæ¥ä¿®å¤ï¼Œclaudecodeæ¥è¯„å®¡ä¿®å¤æ˜¯å¦æœ‰é—®é¢˜ï¼Œå¦‚æœæ²¡é—®é¢˜å°±æ”¾è¡Œã€‚2.  æ˜¯çš„ï¼Œå¼€prå’Œæ›´æ–°prï¼Œclaudecodeå’Œcodexåˆ†åˆ«ç‹¬ç«‹å®Œæ•´ä»£ç è¯„å®¡ï¼Œç„¶åcodexä¿®å¤å’Œè‡ªåŠ¨æµ‹è¯•é€šè¿‡åæäº¤claudecodeè¯„å®¡å¹¶ç»™å‡ºæ”¾è¡Œæ„è§ï¼Œcodexæ ¹æ®æ„è§æ‰§è¡Œã€‚æˆ‘æœ¬åœ°å¼€å‘å®¹å™¨æµ‹è¯•é€šè¿‡å°±ä¼šå‘PRï¼Œç„¶åå°±ä¼šè§¦å‘ä¸Šè¿°å·¥ä½œæµã€‚3ï¼Œ æ˜¯çš„ï¼Œ4.ã€‚ æˆ‘ä»£ç åº“é‡Œé¢æœ‰äº†ä¸ä¸€å®šåˆé€‚5. ä½ çœ‹çœ‹ä»£ç åº“ã€‚å¦å¤–æˆ‘ä¹Ÿå·²ç»åœ¨codexä¸Šåˆ›å»ºäº†ç¯å¢ƒ
ChatGPT said:
æ˜ç™½äº†ï¼Œæˆ‘å°†åŸºäºä½ å½“å‰çš„ storyapp ä»“åº“ç»“æ„ã€å·²æœ‰å·¥ä½œæµé…ç½®ï¼Œä»¥åŠä½ åœ¨ Codex ä¸Šå·²å¯ç”¨çš„ç¯å¢ƒï¼Œè®¾è®¡ä¸€æ¡å®Œæ•´çš„ Dev â†’ PR â†’ Codex ä¿®å¤ + Claude å®¡æŸ¥ â†’ æµ‹è¯•é€šè¿‡ â†’ Claude æ”¾è¡Œ â†’ Codex åˆå¹¶ä¸å‘å¸ƒè¯´æ˜ç”Ÿæˆ çš„è‡ªåŠ¨åŒ– CI/CD æµç¨‹ï¼Œå¹¶æ˜ç¡®ä¸¤è€…èŒè´£åˆ’åˆ†ã€æµç¨‹è¡”æ¥æœºåˆ¶ã€YAML/CLI è„šæœ¬å»ºè®®åŠç¯å¢ƒé…ç½®è§„èŒƒã€‚

æˆ‘ä¼šå°½å¿«æ•´ç†å‡ºå®Œæ•´å»ºè®®ä¸å®ç°è‰æ¡ˆç»™ä½ ã€‚

Research completed in 7m Â· 11 sources Â· 19 searches
æ„å»º Codex ä¸ ClaudeCode ååŒçš„æ™ºèƒ½ PR æµæ°´çº¿

End-to-End AI-Driven CI/CD Pipeline Design for storyapp
In this design, we integrate GitHub Actions workflows with Codex (OpenAI/AI agent) and ClaudeCode (Anthropic) to automate the entire CI/CD process. The pipeline covers everything from local testing and PR creation to AI code review, automatic fixes on test failure, and final merge with release documentation. Below we outline each stage, including the workflow files involved, the triggers, required environment variables, security considerations, and how to update existing workflows/AGENTS.md for this AI-driven process.
1. Automated PR Creation After Local Tests Pass
Trigger & Workflow: After a developer runs local container tests (e.g. via Docker Compose and npm test) and all tests pass, the changes are pushed to the repository. On push to a feature branch (non-main), we configure the CI workflow to automatically open a Pull Request targeting main. We can achieve this by extending the existing CI workflow (.github/workflows/ci.yml)
GitHub
 with a job that runs on successful completion of tests for branch pushes (excluding main). For example:
CI Workflow (ci.yml) â€“ Already triggers on every push to any branch and on PRs
GitHub
. We add a final job (e.g., open-pr) that runs if the workflow was triggered by a branch push (not a PR) and all test jobs succeeded. This job will use the GitHub API or CLI to create a PR. It should have permission pull-requests: write (added in the workflow permissions) to create the PR.
Codex CLI Instruction: No AI is needed just to open the PR â€“ this is a straightforward scripting step. We can use GitHub CLI in a run step, for example:
- name: Create Pull Request
  if: ${{ github.event_name == 'push' && github.ref != 'refs/heads/main' && success() }}
  env:
    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  run: gh pr create --title "Auto PR: ${GITHUB_REF#refs/heads/}" --body "This PR was opened automatically after all local tests passed." --base main --head ${GITHUB_REF#refs/heads/}
This will open a PR from the current branch to main once tests pass. (Alternatively, we could use actions/github-script to call the Octokit API to create a PR.) Environment Variables: This step relies on the built-in GITHUB_TOKEN (with pull_requests: write permission) to authenticate the PR creation. No external AI keys required here. Security Policies: We ensure this auto-PR creation only runs for branches in the same repo (for forked contributions, the GITHUB_TOKEN is read-only and the step will be skipped). We might restrict by actor if necessary (e.g., only repository members trigger it). Also, use a clear PR title and body (no sensitive info). Following the repoâ€™s PR guidelines
GitHub
, the commit messages should use Conventional Commit format (e.g. â€œfeat: ...â€ or â€œfix: ...â€), which will carry into the PR title or can be templated in the body.
2. Dual Independent AI Code Reviews on PR Open/Update
Once the PR is opened (whether manually or by the above automation), two parallel code review workflows kick off â€“ one branded as â€œCodexâ€ and one as â€œClaudeCodeâ€. Both use Anthropicâ€™s Claude (or similar LLMs) under the hood but with different prompts/personas to provide independent perspectives. The user has already configured a ClaudeCode auto-review workflow; we will run both in parallel for thorough review. Trigger: Both workflows trigger on the Pull Request event (types: opened, reopened, synchronized â€“ i.e., new commits â€“ and ready_for_review)
GitHub
. They are scoped to PRs targeting the main branch (as configured in the Claude workflow)
GitHub
.
Workflow A: Codex Review (.github/workflows/codex-review.yml) â€“ This workflow represents an â€œAutomated Review Assistant (Codex)â€. It uses the anthropics/claude-code-action@v1 to post an AI-generated review comment on the PR
GitHub
. The prompt for Codex is tailored to focus on code quality and best practices:
Codex Prompt (Chinese): Instruct the AI to review the PR focusing on:
Code quality & maintainability (TypeScript types, module boundaries, error handling)
Security (input validation, use of secrets, dependency risks)
Performance (frontend rendering efficiency, backend DB/query hotspots)
Testing (adequacy of unit/E2E tests, suggest any additional tests)
Childrenâ€™s app compliance (COPPA, content filtering, parental controls)
Output: Provide feedback in Chinese, in a single PR comment with bullet points marked ğŸ”´ (critical issues), ğŸŸ¡ (suggestions), ğŸŸ¢ (good points). Mention specific files/lines with issues, give actionable improvement advice or examples, and finally a merge recommendation (â€œblockâ€, â€œneutralâ€, or â€œapproveâ€) with a brief rationale
GitHub
GitHub
.
The Codex review workflow ensures it doesnâ€™t run on draft PRs (if: !github.event.pull_request.draft)
GitHub
. It also defines a concurrency group per PR (codex-${{ github.ref }}) to avoid overlapping runs on the same PR
GitHub
 (auto-cancelling previous review runs if a new commit arrives). This workflow requires access to post comments, so we set permissions: pull-requests: write, issues: write for the job
GitHub
. It uses the Claude API via the secrets ANTHROPIC_API_KEY and ANTHROPIC_BASE_URL (if using a custom proxy) for authentication
GitHub
.
Workflow B: ClaudeCode Review (.github/workflows/pr-auto-review.yml) â€“ This is the official Claude Code review process already in place. It likewise triggers on PR open/update (for main branch PRs)
GitHub
. It uses the same anthropics/claude-code-action@v1 but with a more structured prompt and output format. The prompt asks Claude to do a comprehensive code review covering similar points (code quality, security, performance, testing) plus any StoryApp-specific checks (e.g. feature completeness, API security, MongoDB query usage, React best practices)
GitHub
GitHub
. It explicitly reminds the AI about childrenâ€™s app requirements (COPPA compliance, content safety)
GitHub
. ClaudeCode is instructed to output a formatted review report in Chinese, for example:
### ğŸ“‹ ä»£ç è¯„å®¡æŠ¥å‘Š

**PRæ¦‚è¿°**: _Summary of changes..._

#### ğŸ”´ ä¸¥é‡é—®é¢˜
- ... (or "æ— " if none)

#### ğŸŸ¡ å»ºè®®æ”¹è¿›
- ... (or "æ— ")

#### ğŸŸ¢ è‰¯å¥½å®è·µ
- ... (good aspects or "æ— ")

**ğŸ“Š è¯„å®¡è¯„åˆ†**: X/10 åˆ†  
**âœ… å»ºè®®çŠ¶æ€**: æ‰¹å‡†/éœ€è¦ä¿®æ”¹/æ‹’ç»  
**ğŸ’¬ æ€»ä½“å»ºè®®**: _Overall comments..._

*ğŸ¤– æœ¬è¯„å®¡ç”±Claude Codeè‡ªåŠ¨ç”Ÿæˆ | æ—¶é—´: ...*
The workflow ensures the comment follows this template strictly
GitHub
GitHub
. It also uses use_sticky_comment: true and track_progress: true so that if the workflow re-runs, it edits the same comment rather than adding multiples
GitHub
. Like Codex, it only runs on non-draft PRs
GitHub
 and uses secrets for Claude API access
GitHub
. It sets appropriate permissions (contents read, PR/issue write, id-token if needed)
GitHub
. Additionally, this workflow has steps to ensure reliability: for example, an â€œensure-responseâ€ job that checks if Claudeâ€™s comment was posted and if not, posts a fallback message
GitHub
GitHub
, so that the PR is never without a review comment. It also logs the review status for visibility
GitHub
.
Environment Variables: Both review jobs need the Anthropic API Key and possibly a Base URL. According to the projectâ€™s setup, ANTHROPIC_API_KEY is stored in GitHub Secrets, and ANTHROPIC_BASE_URL is set to the GAC platform proxy endpoint
GitHub
. We pass these either via with: parameters or (as the team discovered) via environment variables to avoid warnings (Claude Action v1 doesnâ€™t officially support anthropic_base_url as input)
GitHub
. For example, in the final fix they chose to supply the API key in both with and env for redundancy
GitHub
, and put the base URL in env. We will follow that optimized approach to ensure the action authenticates properly without OIDC issues
GitHub
. Security Policies: Both workflows adhere to least privilege:
They use the repositoryâ€™s GITHUB_TOKEN (scoped to the repo) rather than any external credentials for GitHub operations
GitHub
. The final OIDC fix report confirms that using the default github_token with write access to PRs is the stable solution
GitHub
GitHub
.
Secrets (Claude API keys) are never printed in logs. The action will error if keys are missing or invalid, but the team even added a sanity check step to validate the GitHub token without exposing it
GitHub
.
Reviews on PRs from forks: By default, GitHub does not expose secrets to workflows on forked PRs. This means the Claude/Codex review actions will not run for external contributions (to avoid leaking API keys). Thatâ€™s acceptable for security â€“ maintainers would handle those PRs manually or could choose to run the action manually with appropriate safeguards. We assume most PRs in this project come from internal branches (given the userâ€™s workflow). If needed, we could add an if: to ensure the PR is from the same repo (github.event.pull_request.head.repo.full_name == github.repository) before running the AI reviews, to be explicit.
AGENTS.md Updates: In AGENTS.md, we should document these dual review roles. For example:
Explain that â€œCodexâ€ acts as an automated reviewer focusing on code quality/maintainability, security, etc., giving suggestions with severity labels (ğŸ”´ğŸŸ¡ğŸŸ¢) and a merge recommendation.
Explain that â€œClaudeCodeâ€ provides a structured review report with a score and explicit status (approve/needs changes/reject).
Emphasize that both should remain objective, avoid bias, and not reveal any secret keys or internal URLs in their feedback (just assess the code and content).
We might also add that they should not conflict with each otherâ€™s advice in trivial ways â€“ if they do differ, maintainers will see both viewpoints. Each operates independently, which is by design.
By having two independent AI reviews, we get a more robust code review process on each PR open/update. The PR author can see both comments: one more free-form with line-specific suggestions (Codex) and one a formal report with a checklist and score (ClaudeCode).
3. Continuous Integration (CI) with Automated AI Fix on Failure
In parallel with the AI reviews, the standard CI pipeline runs for the PR. The CI workflow (ci.yml) is already set up to run on pull_request events and on pushes
GitHub
. It covers building and testing of the application across backend, frontend, and end-to-end tests:
The CI has a â€œfast-checksâ€ job for build and unit tests (runs on Node 20, installs dependencies with npm ci, builds shared, backend, frontend, then starts a MongoDB service container for tests, and runs backend Jest tests)
GitHub
GitHub
GitHub
.
If the PR is not a draft, it triggers the â€œdeep-testsâ€ job for full integration tests: it uses Docker Compose to bring up the app and DB in a CI environment (docker-compose.ci.yml), waits for health checks, installs Playwright browsers, then runs the Playwright E2E tests
GitHub
GitHub
. The Playwright HTML report is uploaded as an artifact for debugging if needed
GitHub
. (This aligns with the documented test commands in CLAUDE.md, where they show how to run containerized tests
GitHub
.)
If all tests pass, great â€“ the CI succeeds. But if any test fails (unit or E2E), we want Codex (the AI agent) to automatically attempt to fix the issues and push a commit to the PR. This automates the bug-fix loop that would normally require a human. Hereâ€™s how we design the AI Auto-Fix step: Trigger for Auto-Fix: We use a separate workflow, say ai-auto-fix.yml, that runs on a CI failure. We can leverage the workflow_run trigger: it can listen for the â€œCIâ€ workflow to complete. For example:
on:
  workflow_run:
    workflows: ["CI"]  # Name of the CI workflow
    types: [completed]
Within this, we add a condition to only run if the CI workflow failed (github.event.workflow_run.conclusion == 'failure') and if it was for a PR (we can check github.event.workflow_run.event == 'pull_request'). We also ensure the PR is from our repo (not a fork) so that secrets are available for AI. This gating prevents unauthorized code from external forks from triggering our AI fix with secrets. AI Fix Job Steps: The auto-fix job will do the following (following the â€œAI Bug Fixæµç¨‹â€ that was outlined in the project docs
GitHub
, but now fully automated):
Checkout Code: Use actions/checkout to get the PRâ€™s code at the failing commit
GitHub
.
Setup Environment: Install Node (use actions/setup-node) and any needed dependencies. Possibly run npm ci for both backend and frontend to ensure we can build/test. (The CI already did build/test, but running tests again in this workflow helps gather failure info.)
Run Tests to Collect Failures: Execute the same test suite to reproduce the failure. For example:
Run npm run build and npm test (which runs Playwright E2E by default) or run backend tests separately if needed. We might reuse the docker-compose for a consistent environment. This step should be allowed to fail without stopping the workflow (continue-on-error: true), so we can capture the error output.
Capture logs or test reports. We can direct test output to a file or parse the console output. This provides the error messages, stack traces, or failing test names to feed into the AI.
Analyze & Prompt AI (Codex) for Fix: Now we invoke the AI to generate a fix. We have two provider options:
Anthropic Claude: Since we already use Claude for reviews, we could also prompt Claude to suggest code changes. Claude is good at following complex instructions and code context. We would provide it with a prompt that includes a summary of failing tests (e.g., â€œTest X expected Y but got Z at file.js line 123â€), possibly the relevant code snippets (the functions or lines that failed), and ask for a patch.
OpenAI Codex/GPT-4: The term â€œCodexâ€ might imply using OpenAIâ€™s code model. The project has support for OpenAI API as well
GitHub
. We can choose the best model for code fixes (GPT-4 can produce reliable code fixes). The user has an OpenAI API key secret presumably. In the workflow we can offer a choice or just use one.
For automation, we might use an existing GitHub Action or CLI to interface with the AI. Since the project already uses Claude via the claude-code-action, one approach is to use a separate instance of it in â€œdiffâ€ mode if supported (though the Claude action mainly writes comments, not code to repo). Alternatively, use actions/github-script to call the OpenAI API directly (writing a small NodeJS snippet with OpenAI SDK) or use a community action for OpenAI. To keep it straightforward, assume we call the OpenAI API:
Prepare a prompt like:
â€œThe following tests are failing:\n<include test errors and relevant code context>. Please propose code changes (in diff or clear list form) to fix the failures. Only output the changes to make, no extra explanation.â€
The AIâ€™s response could be a unified diff or instructions. Ideally, we ask for a diff format so we can apply it programmatically. (If using GPT-4, instruct it to output markdown-formatted diff for each file.)
Apply the Fix to Code: Take the AIâ€™s suggested changes and apply them. This could be done by:
If the AI provided a unified diff, save it to a file and use UNIX patch: git apply patch.diff.
If it provided pseudo-code or instructions, we might need a more sophisticated parser. However, since we control the prompt, we can ask for a diff or at least code blocks we can replace. We should double-check the changes donâ€™t introduce new issues or break formatting.
After applying, run git diff to see the changes, and add those to the commit.
Commit and Push the Fix: Use Git config with an bot identity (e.g., an account or just the GITHUB_TOKEN which can push to the branch if itâ€™s in the same repo). Commit message should follow Conventional Commits, e.g. "fix(e2e): AI fix for failing tests" or similar
GitHub
. Pushing the commit will update the PR. The CI workflow (and the review workflows) will automatically be triggered again by this push.
Verify the Fix (Re-run tests): (This could be optional in the fix workflow since the CI will run anyway on the pushed commit. But to be safe, we can run tests again within the fix workflow before pushing.)
If we run tests again in this job and they are still failing, we know the AIâ€™s first fix didnâ€™t work. We might then choose to repeat the AI fix step with more context or bail out. To avoid an infinite loop, we likely limit to one attempt, or maybe two at most, then leave the rest to human if it fails multiple times.
If tests pass locally in the fix workflow, itâ€™s a good sign. We push the commit, and the upstream CI will also run and should pass.
Environment Variables: The auto-fix workflow will need access to AI API keys. If using Claude via the same action, we use ANTHROPIC_API_KEY; if using OpenAI, we need an OPENAI_API_KEY secret configured (the README guides adding OpenAI API key in secrets
GitHub
). We must ensure these secrets are available â€“ by default, workflow_run events do have access to secrets as long as the original event (pull_request) is from the same repo (which weâ€™ve ensured). Also, the Node environment for running tests might need environment variables (e.g., test database URL). The CI already uses a MongoDB service; in the fix job, we can reuse the same Docker Compose for consistency. For example, set MONGODB_URI for tests (point it at localhost or a compose service if we spin one up). We should also include any necessary env (like NODE_ENV=test) similar to CI
GitHub
. Security Policies:
Scoped Secrets: The AI keys are kept in GitHub Secrets. We use them only in this workflow and do not log the raw values. Any AI responses (which might include code) should be reviewed â€“ we should avoid the AI printing out large chunks of secrets or config. In our prompt, we wonâ€™t provide secrets, only code and error messages, so thatâ€™s safe.
Permission to Push: The GITHUB_TOKEN has default push permissions to the repo. However, if branch protection is enabled on main (not sure if feature branches are protected), pushing to a PR branch is usually fine. We should ensure the token is allowed to push to that branch; since the PR branch is user-created, it typically is. If needed, we might use a PAT with repo scope stored as a secret for pushing, but likely unnecessary.
Controlled Invocation: Only trigger auto-fix for trusted code. Because secrets are involved, this runs only for PRs within the repo. If an external forkâ€™s PR fails CI, we do not run auto-fix (to prevent exposing AI API keys). Maintainers would have to manually handle those.
AI Guardrails: In AGENTS.md we should add guidance for Codex when doing auto-fix. For example:
â€œWhen automatically fixing tests, make minimal necessary changes. Do not refactor unrelated code. Ensure all tests pass locally before committing. Follow coding style and project guidelines (lint, formatting, etc.).â€
We should also instruct the agent to use existing functions or patterns rather than introducing new dependencies for a quick fix, unless absolutely needed.
Emphasize safe fixes: e.g., if a test expectation is wrong rather than the code, the AI might suggest updating the test â€“ which could be valid. But we should prefer to fix the code to meet the test unless the test is clearly incorrect. These nuances can be noted in AGENTS.md for the AIâ€™s reference.
By implementing this auto-fix workflow, the pipeline becomes self-healing: CI failures trigger an AI fix attempt. This aligns with the userâ€™s request that â€œif CI fails Codex automatically fixes and re-testsâ€. The documented AI bug fix steps
GitHub
 are essentially automated, eliminating the need for a developer to manually debug immediately.
4. Iterative Cycle: Re-Testing and Re-Review after Fixes
When Codexâ€™s fix is pushed to the PR branch, it kicks off the cycle again for that PR:
The CI workflow will re-run on the updated commit. Ideally, the tests are now green (if the AI fix was successful). If, by chance, tests fail again, the auto-fix workflow could trigger again. (It might be wise to allow only one loop or we risk an AI loop. We can include a condition like github.run_attempt == 1 in the auto-fix trigger to only auto-fix on the first failure, so that it doesnâ€™t keep retrying endlessly. This was a pattern used to avoid infinite loops in Claude Action as well
GitHub
.)
The AI review workflows (Codex review and ClaudeCode review) will trigger again on the PR synchronize event (any push updates the PR)
GitHub
GitHub
. Because we enabled use_sticky_comment:true, each will edit their previous comment with updated analysis rather than creating new comments. This means the AI reviews will now evaluate the PR including the new fixes:
If the fix resolved the issues, the AI comments might change status from â€œneeds changesâ€ to â€œapproveâ€ (for ClaudeCodeâ€™s å»ºè®®çŠ¶æ€ field, it might change from "éœ€è¦ä¿®æ”¹" to "æ‰¹å‡†"). The Codex reviewer might remove previous ğŸ”´ issues or mark them resolved and lean toward approving if no new issues are found.
If the fix introduced new problems or still fails some criteria, the AI reviews will flag them. In practice, since CI is now passing, most critical issues should be resolved, so likely the AI reviews will be satisfied.
This iterative loop can repeat if necessary (though ideally just once). The developer can monitor the PR: they will see that after their initial push, the AI might have pushed 1-2 additional commits to fix tests, and the AI reviews update accordingly. The pipeline basically acts as an autonomous developer+reviewer team that brings the PR to a mergeable state. Note: We must ensure that multiple concurrent pushes donâ€™t mess up the process:
If the human developer pushes another commit while Codex is working, our concurrency controls in workflows help. The CI uses concurrency by workflow+ref, so it cancels previous CI runs on that branch
GitHub
. The Codex review also cancels its prior run on new commit
GitHub
. The Claude reviewâ€™s ensure-response logic handles any cases of missing comment. So things should stabilize after the last push.
The auto-fix workflow, using workflow_run, triggers per CI run completion. If a developerâ€™s manual commit fixes the issue before Codex, then CI might succeed and the auto-fix (which triggers only on failure) simply wonâ€™t run. Thereâ€™s no harm in that â€“ the pipeline would proceed normally.
AGENTS.md: Itâ€™s worth adding a note for the Claude reviewer agent: that after an AI fix commit, some issues may already be resolved, so it should focus on any remaining problems. It should not keep demanding the same change if it sees it was fixed. (Claudeâ€™s analysis of diff might help; though the action likely provides the full PR diff context automatically.) Typically, the Claude Code action diff-checks might highlight recent changes. We trust the action to feed the PRâ€™s latest state. Just ensure in AGENTS.md that Claudeâ€™s review should always reflect the latest code and test status (which it will, since it runs after CI and we can even supply test results summary in the prompt if desired). At this stage, we now have a PR where:
CI is green (tests passing),
Codex and ClaudeCode have given their reviews. Ideally they are positive (or at worst neutral) since the issues were fixed.
This leads to the final stage: merging and release artifacts.
5. Automatic Merge & Release Documentation Generation
When the PR is fully approved (implicitly by the AI reviews) and tests are passing, we want Codex to automatically merge the PR and generate release notes/artifacts like the CHANGELOG, architecture diagram, and risk list. Merge Trigger/Condition: We need to programmatically determine that â€œreview passes.â€ Since we have two AI review comments, we should ensure neither is flagging blocking issues:
The ClaudeCode review comment includes a "å»ºè®®çŠ¶æ€" field with values â€œæ‰¹å‡†â€ (approve), â€œéœ€è¦ä¿®æ”¹â€ (needs changes), or â€œæ‹’ç»â€ (reject)
GitHub
. We can parse this comment via the GitHub API.
The Codex review comment likely ends with a merge recommendation (â€œapproveâ€ or â€œblockâ€)
GitHub
.
Additionally, we ensure the CI status is success (which we know by this point).
We might also consider requiring a minimum review score (Claudeâ€™s è¯„åˆ† out of 10). If the score is very low, maybe we hold off. But since this is automated, the simplest check is the textual recommendation.
Implementing Auto-Merge: There are a couple ways:
Add a step in an existing workflow after reviews to handle merging. For example, in the Claude review workflow, after posting the comment and summary, we could add a job that depends on the review job and checks the content of the comment. However, the Claude workflow doesnâ€™t inherently know about the Codex comment. Similarly, Codex workflow doesnâ€™t know about Claudeâ€™s.
A better approach is a dedicated merge workflow that triggers on PR events or on a schedule to check merge criteria. A simple way: trigger on the PR labeled event. We could have the AI label the PR when it approves (though the current action doesnâ€™t add labels, only comments). Alternatively, trigger on workflow_run of the review workflow completion. For example, once the Claude review workflow completes, we start a â€œmerge-checkâ€ job that reads both comments.
One design: Use the Claude review workflowâ€™s summary job. The Claude auto-review (pr-auto-review.yml) has a final job review-summary that currently just logs a status message
GitHub
. We can extend or follow that with a step to attempt auto-merge:
It can call the GitHub API to fetch the PR comments (similar to how ensure-response did)
GitHub
, filter for the Codex and Claude bot comments, and parse them. For example, search for â€œå»ºè®®çŠ¶æ€: æ‰¹å‡†â€ and â€œapproveâ€ keywords.
Or simpler: require that Claudeâ€™s å»ºè®®çŠ¶æ€ == æ‰¹å‡† (since Claudeâ€™s criteria is quite comprehensive). If Claude says "æ‰¹å‡†", we consider the PR approved. (We might also ensure Codex didnâ€™t explicitly say â€œblockâ€. If Codex had major concerns, it likely would have said block.)
Then verify CI status. We can either trust that CI is done (since auto-fix wouldnâ€™t run if CI was already success). But for safety, check that the latest commit status is success (GitHub provides status or check-runs API).
If all good, use an API call to merge the PR. We can use octokit.rest.pulls.merge via github-script, or invoke gh pr merge. For a clean history, probably a squash merge or rebase merge could be chosen; but the user did not specify, so a normal merge commit is fine or squash to keep one commit. Weâ€™ll assume a merge commit.
Example using github-script to merge:
- name: Merge PR
  if: ${{ steps.analysis.outputs.can_merge == 'true' }}
  uses: actions/github-script@v7
  with:
    script: |
      await github.rest.pulls.merge({
        owner: context.repo.owner,
        repo: context.repo.repo,
        pull_number: ${context.issue.number},
        merge_method: 'squash'
      });
We must ensure the GitHub token has the right to merge. If branch protection requires e.g. 1 approving human review, our botsâ€™ comments might not count. If branch protection is in place, this auto-merge could be blocked. Since the current process was manual merge
GitHub
, they might not have branch protection enforced yet (their next-step suggestions included adding branch protection)
GitHub
. For now, we assume the action can merge (the GITHUB_TOKEN of a workflow run triggered by a repository write member will have push/merge rights). We should run this in the main repository context (not from a fork). After this step, the PR is merged into main ğŸ‰. This triggers any workflows on push to main. Generating Release Artifacts (CHANGELOG, Diagram, Risks): Once merged, we need to produce the release notes. We have a few options:
On merge (still within the PR workflow): We could continue the job after the merge API call to gather info and create artifacts.
On push to main: The repository already has a Docker build workflow on push to master
GitHub
. We can create another workflow or extend an existing one to generate release documentation whenever something is merged to main.
A convenient approach is to use the GitHub Releases feature to publish release notes. Possibly we tie it to version tags. The CICD setup doc suggests automating release notes on tagging
GitHub
. We could integrate with that:
Use Conventional Commit messages from all commits in the PR to build a CHANGELOG entry (since the user follows Conventional Commits
GitHub
, we can categorize features, fixes, etc. automatically).
However, if we merge every PR individually, we might not tag every single PR as a release. Perhaps the user wants a release per PR. If so, we can auto-increment a version or use date-based versioning.
To keep it simple, we can generate the release content now and either:
Create a draft GitHub Release named after this PR (or bump version) with the content.
Or commit a CHANGELOG.md update to the repo.
Given the request explicitly mentions generating å‘å¸ƒè¯´æ˜ (release notes) including changelog, diagram, risks, we should make these easily visible. Letâ€™s outline what each contains and how to generate them via Codex:
CHANGELOG: Summarize the changes in this PR. We have context like PR title, description, and commit messages. Codex can combine these to produce a concise list of changes. For example, if commit messages were conventional, list each as a bullet under categories (Features, Fixes, etc.). If not, just summarize in a paragraph. This can be in Chinese if the project documentation is Chinese. (Likely yes, since internal docs are Chinese.)
Architecture Diagram: Use AI to describe or illustrate the updated architecture. Since generating an actual image automatically is tricky, we can have Codex output a Mermaid diagram code or textual diagram:
E.g., a Mermaid flowchart or sequence diagram showing components (Frontend, Backend, Database, External API). The prompt could be: â€œUpdate the high-level architecture diagram of the system, in Mermaid syntax, highlighting any new component or changes introduced by this PR. If no major arch change, still present the overall architecture for completeness.â€
Mermaid code in the release notes can render on GitHub (Mermaid is supported in markdown). Alternatively, Codex could output an ASCII art diagram or a textual description of the architecture if Mermaid is not desired.
Another option: if there was already an architecture image in docs, we could attempt to update it, but thatâ€™s complex. We stick to generating a new diagram snippet.
Risk List: Ask Codex to enumerate potential risks or things to watch out for after this change is deployed. For example:
Performance hotspots (e.g., â€œthe new image processing might use more memoryâ€),
Security implications (e.g., â€œadded dependency X, ensure itâ€™s trustedâ€),
Known limitations or edge cases not covered.
If the PR is small, risk list might be short or generic (â€œNo major risks â€“ all changes are internal refactors.â€). Thatâ€™s fine.
This essentially acts as a mini post-merge review from the AI focusing on deployment and maintenance concerns (very useful for release managers).
Codex CLI for Release Notes: We can implement this in a workflow (perhaps triggered on push to main). For instance, a release-notes job that runs after merge:
Use actions/checkout to get the latest main.
Use actions/github-script or a custom script to gather data: PR title/body (from the merge commit message or by calling GitHub API to get the merged PR details), commit list in the PR, etc.
Construct a prompt for the OpenAI API (Codex/GPT-4) like:
*â€œYou are an AI Release Notes Assistant. We just merged a pull request into the main branch. Here are the details: [PR title/description, list of commits].
Summarize the changes in a CHANGELOG section (in Chinese) using bullet points.
Provide an Architecture section: give a high-level architecture diagram or description in Mermaid markdown if appropriate.
Provide a Risk Assessment section: list any potential risks or important things to note about this release (performance, security, etc.).â€*
Get the AIâ€™s response and then either:
Use the GitHub API to create a new Release with this content (perhaps tag the commit with a version like v1.0.X if versioning is in place; or use the PR number as an identifier). The release notes field will contain the markdown from AI (which can include the diagram and lists).
Also, possibly commit the changelog: e.g., append to a CHANGELOG.md file and commit. The architecture diagram and risk list could be included in a markdown file in docs/ (since .github/workflows/ci.yml ignores docs/** on push
GitHub
, adding docs wonâ€™t retrigger CI). For persistent record, adding to the repo is nice; but it does create an extra commit on main for every PR merge. If thatâ€™s acceptable, we can do it. Alternatively, rely on GitHub Releases as the source of truth for release notes.
The user explicitly said â€œç”Ÿæˆ CHANGELOGã€æ¶æ„å›¾ã€é£é™©æ¸…å•ç­‰å‘å¸ƒç‰© (generate CHANGELOG, architecture diagram, risk list and other release artifacts).â€ This suggests these might be assembled into release documentation. A GitHub Release page is a logical place. However, it might also be intended to store in the repo for reference. We could do both (generate in release, and commit a copy in docs). Environment & Security: To generate release notes, we again use the AI API keys. Since this runs on push to main (a trusted context), secrets are available. No user input here, just code and commit info, so prompt injection isnâ€™t a concern. We must ensure the AI doesnâ€™t hallucinate incorrect details â€“ providing it with the actual commit diff or PR body helps accuracy. Also, verify the architecture diagram syntax (Mermaid) is correct, or it might not render â€“ possibly have a step to lint the Mermaid (there are CLI tools) or just trust the AI if the architecture is simple. AGENTS.md Updates: In AGENTS.md, add a section for Release Artifact Generation guidelines for Codex:
The AI should be instructed to follow a standard format for release notes: e.g. â€œ## Changelogâ€, â€œ## Architectureâ€, â€œ## Risksâ€ sections.
It should keep the language concise and factual. If writing in Chinese (since the project docs are largely Chinese), maintain a professional tone.
For the architecture diagram, note that it should be high-level (donâ€™t try to include every file, just major components) and accurately reflect any new modules or integration introduced by the PR.
For risks, instruct the agent to think like a reviewer: any security implications, performance concerns, edge cases, or things that need monitoring post-deployment should be listed.
Also, ensure the agent does not reveal sensitive info (like internal server IPs or keys) in release notes. Only discuss what's relevant from the code changes.
Finally, after this step, the PR is merged, a new release notes is generated, and the CI/CD pipeline for that change is essentially complete. The Docker build workflow (already existing) will pick up the new commit on main and build/push the container image to GHCR
GitHub
GitHub
. A maintainer can then trigger the deployment workflow manually to push to production when ready (unless we automate that too in the future).
6. Optimizing Workflows & Documentation (AGENTS.md)
To implement the above, we should refine some existing workflow files and the AGENTS.md guidelines:
CI Workflow (ci.yml): Add the PR auto-create job at the end on success. Also, ensure that the CI jobs have appropriate conditions (we might skip E2E on pushes to non-main to save time, but since we want full test before PR, we kept it manual local. However, in our case we are running tests on push anyway then PR again â€“ a bit redundant. We could decide to run full tests only on PR to avoid double load. But the user specifically said local tests pass then PR, so likely the push is after they've run tests locally. We can keep CI on push to do at least a quick check or just trust local and directly PR. It might be simpler to trigger PR creation immediately on push (assuming the developer only pushes after passing tests). For safety, we used CI to gate it though.) Also, if there are any lint or type-check steps to add (the CICD_SETUP.md mentioned lint/typecheck gates
GitHub
), we should include those before tests and also feed failures to AI fix if needed (though AI fix focusing on test failures primarily; lint issues could also be auto-fixed with a formatter or simple suggestions).
Codex Review Workflow: We might rename it or comment in it that it uses Claude under the hood (to avoid confusion, since itâ€™s called Codex but uses claude-code-action). No functional change needed besides making sure the prompt stays up-to-date with project focus. Given we now have a separate Claude review, we might tweak Codexâ€™s prompt to avoid too much overlap. For example, let Codex focus more on maintainability/testing and Claude on security/compliance, etc. But some redundancy is fine (two opinions on same areas).
ClaudeCode Review Workflow: Itâ€™s quite elaborate already. Ensure the anthropic_base_url handling is only via env (to avoid warning)
GitHub
. The final verification report suggests removing the unsupported input to clean up warnings
GitHub
. We implement that. Also, the continue-on-error: ... trick they used
GitHub
 could be kept or adjusted. (They set Claude step to continue-on-error on first run attempt so that if Claude fails, it doesnâ€™t block CI â€“ it posts fallback instead. Thatâ€™s fine.)
Auto-Fix Workflow: This is new; we will document it clearly. Key points: only runs on our CI failure for PRs, uses secrets for AI, and pushes commits. We will note in the workflow file comments that it implements the steps from AI Bug Fix in the docs
GitHub
.
Merge/Release Workflow: We might integrate this with the Claude review workflow as mentioned. Or implement a small separate workflow triggered on push to main (to handle release notes after every merge). Either approach, clearly comment what it does. Perhaps a separate .github/workflows/release-notes.yml triggered on push to main (filtering out dependabot or docs changes if needed):
Trigger: on: push: branches: [main].
Condition: maybe only trigger when the push is a merge commit from a PR (we can detect commit message contains â€œMerge pull requestâ€).
Steps: checkout, collect PR info via GitHub API (the commit or PR number might be in the commit message), generate notes with AI, publish notes (commit or release).
This decouples it from PR context and ensures it runs in an environment where the code is final.
AGENTS.md: Update this file to reflect:
Codex (OpenAI) Agent: roles in auto-fixing and merging. Provide instructions for how it should handle code changes (as discussed, minimal changes, test focus, commit message conventions). Also guidelines for generating release docs (structure, accuracy, no leaking secrets).
Claude Code Agent: clarify its role is solely review; ensure it doesnâ€™t attempt to make code changes. It should focus on identifying issues. Also mention that its approval will be used to gate auto-merge, so it should only approve if truly no major issues remain. This might entail instructing Claude to treat any test failure or critical security issue as a â€œæ‹’ç»â€ or â€œneeds changesâ€, and only give â€œæ‰¹å‡†â€ if code is production-ready. (The prompt already leans that way by asking for block/approve at end
GitHub
 and a score.)
Perhaps add a note that both agents should remain respectful and professional in comments (they do, since prompt is formal).
Keep existing content about keeping changes small, respecting structure
GitHub
, since thatâ€™s relevant for AI as well (AI should not refactor whole codebase in an auto-fix).
If any environment-specific instructions for AI (like â€œdonâ€™t reveal API keysâ€, or â€œif something about DeepSeek API appears, note itâ€), include those.
Summary: With the above changes, the CI/CD pipeline becomes fully automated:
A developerâ€™s push triggers CI and automatically opens a PR if tests passed.
Opening a PR triggers both Codex and ClaudeCode AI reviews independently
GitHub
GitHub
, as well as the CI tests on the PR.
If CI fails, an AI fix workflow activates to correct the code and push a commit
GitHub
. The PR is updated and CI runs again.
Each update triggers the AI reviews again, providing iterative feedback until no critical issues remain.
Once CI is green and AI reviews signal approval, the pipeline automatically merges the PR. Codex then generates the release artifacts: updating the CHANGELOG and producing an architecture diagram and risk assessment for the release (e.g., via a Release note or commit).
The Docker image build workflow picks up the new main commit and builds/deploys as configured (continuing the CD process)
GitHub
.
This end-to-end pipeline fulfills the userâ€™s requirements. It leverages the existing workflows (CI, Claude review) and adds the missing pieces (auto PR, auto-fix, auto-merge, and release note generation) in a cohesive manner. The result is a highly automated CI/CD system where AI agents assist at each stage: code review, test failure fixing, and even documenting the release. Human oversight is still important (one should monitor AI commits and reviews), but the heavy lifting is automated, accelerating the development workflow. Sources:
StoryApp CI workflow (build/tests)
GitHub
GitHub
Codex review workflow (Claude-based, PR trigger, prompt with ğŸ”´ğŸŸ¡ğŸŸ¢ and merge suggestion)
GitHub
GitHub
ClaudeCode auto-review workflow (PR trigger, structured review report template)
GitHub
GitHub
Project docs â€“ AI bug fix flow and CI/CD overview
GitHub
GitHub
Final fix report â€“ Claude action integration and best practices (token usage, env vars, continue-on-error)
GitHub
GitHub
Repository guidelines (commit conventions, PR requirements)
GitHub
 and environment configuration for AI
GitHub
.

Sources


æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶

End-to-End AI-Driven CI/CD Pipeline Design for storyapp
In this design, we integrate GitHub Actions workflows with Codex (OpenAI/AI agent) and ClaudeCode (Anthropic) to automate the entire CI/CD process. The pipeline covers everything from local testing and PR creation to AI code review, automatic fixes on test failure, and final merge with release documentation. Below we outline each stage, including the workflow files involved, the triggers, required environment variables, security considerations, and how to update existing workflows/AGENTS.md for this AI-driven process.
1. Automated PR Creation After Local Tests Pass
Trigger & Workflow: After a developer runs local container tests (e.g. via Docker Compose and npm test) and all tests pass, the changes are pushed to the repository. On push to a feature branch (non-main), we configure the CI workflow to automatically open a Pull Request targeting main. We can achieve this by extending the existing CI workflow (.github/workflows/ci.yml)
GitHub
 with a job that runs on successful completion of tests for branch pushes (excluding main). For example:
CI Workflow (ci.yml) â€“ Already triggers on every push to any branch and on PRs
GitHub
. We add a final job (e.g., open-pr) that runs if the workflow was triggered by a branch push (not a PR) and all test jobs succeeded. This job will use the GitHub API or CLI to create a PR. It should have permission pull-requests: write (added in the workflow permissions) to create the PR.
Codex CLI Instruction: No AI is needed just to open the PR â€“ this is a straightforward scripting step. We can use GitHub CLI in a run step, for example:
- name: Create Pull Request
  if: ${{ github.event_name == 'push' && github.ref != 'refs/heads/main' && success() }}
  env:
    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  run: gh pr create --title "Auto PR: ${GITHUB_REF#refs/heads/}" --body "This PR was opened automatically after all local tests passed." --base main --head ${GITHUB_REF#refs/heads/}
This will open a PR from the current branch to main once tests pass. (Alternatively, we could use actions/github-script to call the Octokit API to create a PR.) Environment Variables: This step relies on the built-in GITHUB_TOKEN (with pull_requests: write permission) to authenticate the PR creation. No external AI keys required here. Security Policies: We ensure this auto-PR creation only runs for branches in the same repo (for forked contributions, the GITHUB_TOKEN is read-only and the step will be skipped). We might restrict by actor if necessary (e.g., only repository members trigger it). Also, use a clear PR title and body (no sensitive info). Following the repoâ€™s PR guidelines
GitHub
, the commit messages should use Conventional Commit format (e.g. â€œfeat: ...â€ or â€œfix: ...â€), which will carry into the PR title or can be templated in the body.
2. Dual Independent AI Code Reviews on PR Open/Update
Once the PR is opened (whether manually or by the above automation), two parallel code review workflows kick off â€“ one branded as â€œCodexâ€ and one as â€œClaudeCodeâ€. Both use Anthropicâ€™s Claude (or similar LLMs) under the hood but with different prompts/personas to provide independent perspectives. The user has already configured a ClaudeCode auto-review workflow; we will run both in parallel for thorough review. Trigger: Both workflows trigger on the Pull Request event (types: opened, reopened, synchronized â€“ i.e., new commits â€“ and ready_for_review)
GitHub
. They are scoped to PRs targeting the main branch (as configured in the Claude workflow)
GitHub
.
Workflow A: Codex Review (.github/workflows/codex-review.yml) â€“ This workflow represents an â€œAutomated Review Assistant (Codex)â€. It uses the anthropics/claude-code-action@v1 to post an AI-generated review comment on the PR
GitHub
. The prompt for Codex is tailored to focus on code quality and best practices:
Codex Prompt (Chinese): Instruct the AI to review the PR focusing on:
Code quality & maintainability (TypeScript types, module boundaries, error handling)
Security (input validation, use of secrets, dependency risks)
Performance (frontend rendering efficiency, backend DB/query hotspots)
Testing (adequacy of unit/E2E tests, suggest any additional tests)
Childrenâ€™s app compliance (COPPA, content filtering, parental controls)
Output: Provide feedback in Chinese, in a single PR comment with bullet points marked ğŸ”´ (critical issues), ğŸŸ¡ (suggestions), ğŸŸ¢ (good points). Mention specific files/lines with issues, give actionable improvement advice or examples, and finally a merge recommendation (â€œblockâ€, â€œneutralâ€, or â€œapproveâ€) with a brief rationale
GitHub
GitHub
.
The Codex review workflow ensures it doesnâ€™t run on draft PRs (if: !github.event.pull_request.draft)
GitHub
. It also defines a concurrency group per PR (codex-${{ github.ref }}) to avoid overlapping runs on the same PR
GitHub
 (auto-cancelling previous review runs if a new commit arrives). This workflow requires access to post comments, so we set permissions: pull-requests: write, issues: write for the job
GitHub
. It uses the Claude API via the secrets ANTHROPIC_API_KEY and ANTHROPIC_BASE_URL (if using a custom proxy) for authentication
GitHub
.
Workflow B: ClaudeCode Review (.github/workflows/pr-auto-review.yml) â€“ This is the official Claude Code review process already in place. It likewise triggers on PR open/update (for main branch PRs)
GitHub
. It uses the same anthropics/claude-code-action@v1 but with a more structured prompt and output format. The prompt asks Claude to do a comprehensive code review covering similar points (code quality, security, performance, testing) plus any StoryApp-specific checks (e.g. feature completeness, API security, MongoDB query usage, React best practices)
GitHub
GitHub
. It explicitly reminds the AI about childrenâ€™s app requirements (COPPA compliance, content safety)
GitHub
. ClaudeCode is instructed to output a formatted review report in Chinese, for example:
### ğŸ“‹ ä»£ç è¯„å®¡æŠ¥å‘Š

**PRæ¦‚è¿°**: _Summary of changes..._

#### ğŸ”´ ä¸¥é‡é—®é¢˜
- ... (or "æ— " if none)

#### ğŸŸ¡ å»ºè®®æ”¹è¿›
- ... (or "æ— ")

#### ğŸŸ¢ è‰¯å¥½å®è·µ
- ... (good aspects or "æ— ")

**ğŸ“Š è¯„å®¡è¯„åˆ†**: X/10 åˆ†  
**âœ… å»ºè®®çŠ¶æ€**: æ‰¹å‡†/éœ€è¦ä¿®æ”¹/æ‹’ç»  
**ğŸ’¬ æ€»ä½“å»ºè®®**: _Overall comments..._

*ğŸ¤– æœ¬è¯„å®¡ç”±Claude Codeè‡ªåŠ¨ç”Ÿæˆ | æ—¶é—´: ...*
The workflow ensures the comment follows this template strictly
GitHub
GitHub
. It also uses use_sticky_comment: true and track_progress: true so that if the workflow re-runs, it edits the same comment rather than adding multiples
GitHub
. Like Codex, it only runs on non-draft PRs
GitHub
 and uses secrets for Claude API access
GitHub
. It sets appropriate permissions (contents read, PR/issue write, id-token if needed)
GitHub
. Additionally, this workflow has steps to ensure reliability: for example, an â€œensure-responseâ€ job that checks if Claudeâ€™s comment was posted and if not, posts a fallback message
GitHub
GitHub
, so that the PR is never without a review comment. It also logs the review status for visibility
GitHub
.
Environment Variables: Both review jobs need the Anthropic API Key and possibly a Base URL. According to the projectâ€™s setup, ANTHROPIC_API_KEY is stored in GitHub Secrets, and ANTHROPIC_BASE_URL is set to the GAC platform proxy endpoint
GitHub
. We pass these either via with: parameters or (as the team discovered) via environment variables to avoid warnings (Claude Action v1 doesnâ€™t officially support anthropic_base_url as input)
GitHub
. For example, in the final fix they chose to supply the API key in both with and env for redundancy
GitHub
, and put the base URL in env. We will follow that optimized approach to ensure the action authenticates properly without OIDC issues
GitHub
. Security Policies: Both workflows adhere to least privilege:
They use the repositoryâ€™s GITHUB_TOKEN (scoped to the repo) rather than any external credentials for GitHub operations
GitHub
. The final OIDC fix report confirms that using the default github_token with write access to PRs is the stable solution
GitHub
GitHub
.
Secrets (Claude API keys) are never printed in logs. The action will error if keys are missing or invalid, but the team even added a sanity check step to validate the GitHub token without exposing it
GitHub
.
Reviews on PRs from forks: By default, GitHub does not expose secrets to workflows on forked PRs. This means the Claude/Codex review actions will not run for external contributions (to avoid leaking API keys). Thatâ€™s acceptable for security â€“ maintainers would handle those PRs manually or could choose to run the action manually with appropriate safeguards. We assume most PRs in this project come from internal branches (given the userâ€™s workflow). If needed, we could add an if: to ensure the PR is from the same repo (github.event.pull_request.head.repo.full_name == github.repository) before running the AI reviews, to be explicit.
AGENTS.md Updates: In AGENTS.md, we should document these dual review roles. For example:
Explain that â€œCodexâ€ acts as an automated reviewer focusing on code quality/maintainability, security, etc., giving suggestions with severity labels (ğŸ”´ğŸŸ¡ğŸŸ¢) and a merge recommendation.
Explain that â€œClaudeCodeâ€ provides a structured review report with a score and explicit status (approve/needs changes/reject).
Emphasize that both should remain objective, avoid bias, and not reveal any secret keys or internal URLs in their feedback (just assess the code and content).
We might also add that they should not conflict with each otherâ€™s advice in trivial ways â€“ if they do differ, maintainers will see both viewpoints. Each operates independently, which is by design.
By having two independent AI reviews, we get a more robust code review process on each PR open/update. The PR author can see both comments: one more free-form with line-specific suggestions (Codex) and one a formal report with a checklist and score (ClaudeCode).
3. Continuous Integration (CI) with Automated AI Fix on Failure
In parallel with the AI reviews, the standard CI pipeline runs for the PR. The CI workflow (ci.yml) is already set up to run on pull_request events and on pushes
GitHub
. It covers building and testing of the application across backend, frontend, and end-to-end tests:
The CI has a â€œfast-checksâ€ job for build and unit tests (runs on Node 20, installs dependencies with npm ci, builds shared, backend, frontend, then starts a MongoDB service container for tests, and runs backend Jest tests)
GitHub
GitHub
GitHub
.
If the PR is not a draft, it triggers the â€œdeep-testsâ€ job for full integration tests: it uses Docker Compose to bring up the app and DB in a CI environment (docker-compose.ci.yml), waits for health checks, installs Playwright browsers, then runs the Playwright E2E tests
GitHub
GitHub
. The Playwright HTML report is uploaded as an artifact for debugging if needed
GitHub
. (This aligns with the documented test commands in CLAUDE.md, where they show how to run containerized tests
GitHub
.)
If all tests pass, great â€“ the CI succeeds. But if any test fails (unit or E2E), we want Codex (the AI agent) to automatically attempt to fix the issues and push a commit to the PR. This automates the bug-fix loop that would normally require a human. Hereâ€™s how we design the AI Auto-Fix step: Trigger for Auto-Fix: We use a separate workflow, say ai-auto-fix.yml, that runs on a CI failure. We can leverage the workflow_run trigger: it can listen for the â€œCIâ€ workflow to complete. For example:
on:
  workflow_run:
    workflows: ["CI"]  # Name of the CI workflow
    types: [completed]
Within this, we add a condition to only run if the CI workflow failed (github.event.workflow_run.conclusion == 'failure') and if it was for a PR (we can check github.event.workflow_run.event == 'pull_request'). We also ensure the PR is from our repo (not a fork) so that secrets are available for AI. This gating prevents unauthorized code from external forks from triggering our AI fix with secrets. AI Fix Job Steps: The auto-fix job will do the following (following the â€œAI Bug Fixæµç¨‹â€ that was outlined in the project docs
GitHub
, but now fully automated):
Checkout Code: Use actions/checkout to get the PRâ€™s code at the failing commit
GitHub
.
Setup Environment: Install Node (use actions/setup-node) and any needed dependencies. Possibly run npm ci for both backend and frontend to ensure we can build/test. (The CI already did build/test, but running tests again in this workflow helps gather failure info.)
Run Tests to Collect Failures: Execute the same test suite to reproduce the failure. For example:
Run npm run build and npm test (which runs Playwright E2E by default) or run backend tests separately if needed. We might reuse the docker-compose for a consistent environment. This step should be allowed to fail without stopping the workflow (continue-on-error: true), so we can capture the error output.
Capture logs or test reports. We can direct test output to a file or parse the console output. This provides the error messages, stack traces, or failing test names to feed into the AI.
Analyze & Prompt AI (Codex) for Fix: Now we invoke the AI to generate a fix. We have two provider options:
Anthropic Claude: Since we already use Claude for reviews, we could also prompt Claude to suggest code changes. Claude is good at following complex instructions and code context. We would provide it with a prompt that includes a summary of failing tests (e.g., â€œTest X expected Y but got Z at file.js line 123â€), possibly the relevant code snippets (the functions or lines that failed), and ask for a patch.
OpenAI Codex/GPT-4: The term â€œCodexâ€ might imply using OpenAIâ€™s code model. The project has support for OpenAI API as well
GitHub
. We can choose the best model for code fixes (GPT-4 can produce reliable code fixes). The user has an OpenAI API key secret presumably. In the workflow we can offer a choice or just use one.
For automation, we might use an existing GitHub Action or CLI to interface with the AI. Since the project already uses Claude via the claude-code-action, one approach is to use a separate instance of it in â€œdiffâ€ mode if supported (though the Claude action mainly writes comments, not code to repo). Alternatively, use actions/github-script to call the OpenAI API directly (writing a small NodeJS snippet with OpenAI SDK) or use a community action for OpenAI. To keep it straightforward, assume we call the OpenAI API:
Prepare a prompt like:
â€œThe following tests are failing:\n<include test errors and relevant code context>. Please propose code changes (in diff or clear list form) to fix the failures. Only output the changes to make, no extra explanation.â€
The AIâ€™s response could be a unified diff or instructions. Ideally, we ask for a diff format so we can apply it programmatically. (If using GPT-4, instruct it to output markdown-formatted diff for each file.)
Apply the Fix to Code: Take the AIâ€™s suggested changes and apply them. This could be done by:
If the AI provided a unified diff, save it to a file and use UNIX patch: git apply patch.diff.
If it provided pseudo-code or instructions, we might need a more sophisticated parser. However, since we control the prompt, we can ask for a diff or at least code blocks we can replace. We should double-check the changes donâ€™t introduce new issues or break formatting.
After applying, run git diff to see the changes, and add those to the commit.
Commit and Push the Fix: Use Git config with an bot identity (e.g., an account or just the GITHUB_TOKEN which can push to the branch if itâ€™s in the same repo). Commit message should follow Conventional Commits, e.g. "fix(e2e): AI fix for failing tests" or similar
GitHub
. Pushing the commit will update the PR. The CI workflow (and the review workflows) will automatically be triggered again by this push.
Verify the Fix (Re-run tests): (This could be optional in the fix workflow since the CI will run anyway on the pushed commit. But to be safe, we can run tests again within the fix workflow before pushing.)
If we run tests again in this job and they are still failing, we know the AIâ€™s first fix didnâ€™t work. We might then choose to repeat the AI fix step with more context or bail out. To avoid an infinite loop, we likely limit to one attempt, or maybe two at most, then leave the rest to human if it fails multiple times.
If tests pass locally in the fix workflow, itâ€™s a good sign. We push the commit, and the upstream CI will also run and should pass.
Environment Variables: The auto-fix workflow will need access to AI API keys. If using Claude via the same action, we use ANTHROPIC_API_KEY; if using OpenAI, we need an OPENAI_API_KEY secret configured (the README guides adding OpenAI API key in secrets
GitHub
). We must ensure these secrets are available â€“ by default, workflow_run events do have access to secrets as long as the original event (pull_request) is from the same repo (which weâ€™ve ensured). Also, the Node environment for running tests might need environment variables (e.g., test database URL). The CI already uses a MongoDB service; in the fix job, we can reuse the same Docker Compose for consistency. For example, set MONGODB_URI for tests (point it at localhost or a compose service if we spin one up). We should also include any necessary env (like NODE_ENV=test) similar to CI
GitHub
. Security Policies:
Scoped Secrets: The AI keys are kept in GitHub Secrets. We use them only in this workflow and do not log the raw values. Any AI responses (which might include code) should be reviewed â€“ we should avoid the AI printing out large chunks of secrets or config. In our prompt, we wonâ€™t provide secrets, only code and error messages, so thatâ€™s safe.
Permission to Push: The GITHUB_TOKEN has default push permissions to the repo. However, if branch protection is enabled on main (not sure if feature branches are protected), pushing to a PR branch is usually fine. We should ensure the token is allowed to push to that branch; since the PR branch is user-created, it typically is. If needed, we might use a PAT with repo scope stored as a secret for pushing, but likely unnecessary.
Controlled Invocation: Only trigger auto-fix for trusted code. Because secrets are involved, this runs only for PRs within the repo. If an external forkâ€™s PR fails CI, we do not run auto-fix (to prevent exposing AI API keys). Maintainers would have to manually handle those.
AI Guardrails: In AGENTS.md we should add guidance for Codex when doing auto-fix. For example:
â€œWhen automatically fixing tests, make minimal necessary changes. Do not refactor unrelated code. Ensure all tests pass locally before committing. Follow coding style and project guidelines (lint, formatting, etc.).â€
We should also instruct the agent to use existing functions or patterns rather than introducing new dependencies for a quick fix, unless absolutely needed.
Emphasize safe fixes: e.g., if a test expectation is wrong rather than the code, the AI might suggest updating the test â€“ which could be valid. But we should prefer to fix the code to meet the test unless the test is clearly incorrect. These nuances can be noted in AGENTS.md for the AIâ€™s reference.
By implementing this auto-fix workflow, the pipeline becomes self-healing: CI failures trigger an AI fix attempt. This aligns with the userâ€™s request that â€œif CI fails Codex automatically fixes and re-testsâ€. The documented AI bug fix steps
GitHub
 are essentially automated, eliminating the need for a developer to manually debug immediately.
4. Iterative Cycle: Re-Testing and Re-Review after Fixes
When Codexâ€™s fix is pushed to the PR branch, it kicks off the cycle again for that PR:
The CI workflow will re-run on the updated commit. Ideally, the tests are now green (if the AI fix was successful). If, by chance, tests fail again, the auto-fix workflow could trigger again. (It might be wise to allow only one loop or we risk an AI loop. We can include a condition like github.run_attempt == 1 in the auto-fix trigger to only auto-fix on the first failure, so that it doesnâ€™t keep retrying endlessly. This was a pattern used to avoid infinite loops in Claude Action as well
GitHub
.)
The AI review workflows (Codex review and ClaudeCode review) will trigger again on the PR synchronize event (any push updates the PR)
GitHub
GitHub
. Because we enabled use_sticky_comment:true, each will edit their previous comment with updated analysis rather than creating new comments. This means the AI reviews will now evaluate the PR including the new fixes:
If the fix resolved the issues, the AI comments might change status from â€œneeds changesâ€ to â€œapproveâ€ (for ClaudeCodeâ€™s å»ºè®®çŠ¶æ€ field, it might change from "éœ€è¦ä¿®æ”¹" to "æ‰¹å‡†"). The Codex reviewer might remove previous ğŸ”´ issues or mark them resolved and lean toward approving if no new issues are found.
If the fix introduced new problems or still fails some criteria, the AI reviews will flag them. In practice, since CI is now passing, most critical issues should be resolved, so likely the AI reviews will be satisfied.
This iterative loop can repeat if necessary (though ideally just once). The developer can monitor the PR: they will see that after their initial push, the AI might have pushed 1-2 additional commits to fix tests, and the AI reviews update accordingly. The pipeline basically acts as an autonomous developer+reviewer team that brings the PR to a mergeable state. Note: We must ensure that multiple concurrent pushes donâ€™t mess up the process:
If the human developer pushes another commit while Codex is working, our concurrency controls in workflows help. The CI uses concurrency by workflow+ref, so it cancels previous CI runs on that branch
GitHub
. The Codex review also cancels its prior run on new commit
GitHub
. The Claude reviewâ€™s ensure-response logic handles any cases of missing comment. So things should stabilize after the last push.
The auto-fix workflow, using workflow_run, triggers per CI run completion. If a developerâ€™s manual commit fixes the issue before Codex, then CI might succeed and the auto-fix (which triggers only on failure) simply wonâ€™t run. Thereâ€™s no harm in that â€“ the pipeline would proceed normally.
AGENTS.md: Itâ€™s worth adding a note for the Claude reviewer agent: that after an AI fix commit, some issues may already be resolved, so it should focus on any remaining problems. It should not keep demanding the same change if it sees it was fixed. (Claudeâ€™s analysis of diff might help; though the action likely provides the full PR diff context automatically.) Typically, the Claude Code action diff-checks might highlight recent changes. We trust the action to feed the PRâ€™s latest state. Just ensure in AGENTS.md that Claudeâ€™s review should always reflect the latest code and test status (which it will, since it runs after CI and we can even supply test results summary in the prompt if desired). At this stage, we now have a PR where:
CI is green (tests passing),
Codex and ClaudeCode have given their reviews. Ideally they are positive (or at worst neutral) since the issues were fixed.
This leads to the final stage: merging and release artifacts.
5. Automatic Merge & Release Documentation Generation
When the PR is fully approved (implicitly by the AI reviews) and tests are passing, we want Codex to automatically merge the PR and generate release notes/artifacts like the CHANGELOG, architecture diagram, and risk list. Merge Trigger/Condition: We need to programmatically determine that â€œreview passes.â€ Since we have two AI review comments, we should ensure neither is flagging blocking issues:
The ClaudeCode review comment includes a "å»ºè®®çŠ¶æ€" field with values â€œæ‰¹å‡†â€ (approve), â€œéœ€è¦ä¿®æ”¹â€ (needs changes), or â€œæ‹’ç»â€ (reject)
GitHub
. We can parse this comment via the GitHub API.
The Codex review comment likely ends with a merge recommendation (â€œapproveâ€ or â€œblockâ€)
GitHub
.
Additionally, we ensure the CI status is success (which we know by this point).
We might also consider requiring a minimum review score (Claudeâ€™s è¯„åˆ† out of 10). If the score is very low, maybe we hold off. But since this is automated, the simplest check is the textual recommendation.
Implementing Auto-Merge: There are a couple ways:
Add a step in an existing workflow after reviews to handle merging. For example, in the Claude review workflow, after posting the comment and summary, we could add a job that depends on the review job and checks the content of the comment. However, the Claude workflow doesnâ€™t inherently know about the Codex comment. Similarly, Codex workflow doesnâ€™t know about Claudeâ€™s.
A better approach is a dedicated merge workflow that triggers on PR events or on a schedule to check merge criteria. A simple way: trigger on the PR labeled event. We could have the AI label the PR when it approves (though the current action doesnâ€™t add labels, only comments). Alternatively, trigger on workflow_run of the review workflow completion. For example, once the Claude review workflow completes, we start a â€œmerge-checkâ€ job that reads both comments.
One design: Use the Claude review workflowâ€™s summary job. The Claude auto-review (pr-auto-review.yml) has a final job review-summary that currently just logs a status message
GitHub
. We can extend or follow that with a step to attempt auto-merge:
It can call the GitHub API to fetch the PR comments (similar to how ensure-response did)
GitHub
, filter for the Codex and Claude bot comments, and parse them. For example, search for â€œå»ºè®®çŠ¶æ€: æ‰¹å‡†â€ and â€œapproveâ€ keywords.
Or simpler: require that Claudeâ€™s å»ºè®®çŠ¶æ€ == æ‰¹å‡† (since Claudeâ€™s criteria is quite comprehensive). If Claude says "æ‰¹å‡†", we consider the PR approved. (We might also ensure Codex didnâ€™t explicitly say â€œblockâ€. If Codex had major concerns, it likely would have said block.)
Then verify CI status. We can either trust that CI is done (since auto-fix wouldnâ€™t run if CI was already success). But for safety, check that the latest commit status is success (GitHub provides status or check-runs API).
If all good, use an API call to merge the PR. We can use octokit.rest.pulls.merge via github-script, or invoke gh pr merge. For a clean history, probably a squash merge or rebase merge could be chosen; but the user did not specify, so a normal merge commit is fine or squash to keep one commit. Weâ€™ll assume a merge commit.
Example using github-script to merge:
- name: Merge PR
  if: ${{ steps.analysis.outputs.can_merge == 'true' }}
  uses: actions/github-script@v7
  with:
    script: |
      await github.rest.pulls.merge({
        owner: context.repo.owner,
        repo: context.repo.repo,
        pull_number: ${context.issue.number},
        merge_method: 'squash'
      });
We must ensure the GitHub token has the right to merge. If branch protection requires e.g. 1 approving human review, our botsâ€™ comments might not count. If branch protection is in place, this auto-merge could be blocked. Since the current process was manual merge
GitHub
, they might not have branch protection enforced yet (their next-step suggestions included adding branch protection)
GitHub
. For now, we assume the action can merge (the GITHUB_TOKEN of a workflow run triggered by a repository write member will have push/merge rights). We should run this in the main repository context (not from a fork). After this step, the PR is merged into main ğŸ‰. This triggers any workflows on push to main. Generating Release Artifacts (CHANGELOG, Diagram, Risks): Once merged, we need to produce the release notes. We have a few options:
On merge (still within the PR workflow): We could continue the job after the merge API call to gather info and create artifacts.
On push to main: The repository already has a Docker build workflow on push to master
GitHub
. We can create another workflow or extend an existing one to generate release documentation whenever something is merged to main.
A convenient approach is to use the GitHub Releases feature to publish release notes. Possibly we tie it to version tags. The CICD setup doc suggests automating release notes on tagging
GitHub
. We could integrate with that:
Use Conventional Commit messages from all commits in the PR to build a CHANGELOG entry (since the user follows Conventional Commits
GitHub
, we can categorize features, fixes, etc. automatically).
However, if we merge every PR individually, we might not tag every single PR as a release. Perhaps the user wants a release per PR. If so, we can auto-increment a version or use date-based versioning.
To keep it simple, we can generate the release content now and either:
Create a draft GitHub Release named after this PR (or bump version) with the content.
Or commit a CHANGELOG.md update to the repo.
Given the request explicitly mentions generating å‘å¸ƒè¯´æ˜ (release notes) including changelog, diagram, risks, we should make these easily visible. Letâ€™s outline what each contains and how to generate them via Codex:
CHANGELOG: Summarize the changes in this PR. We have context like PR title, description, and commit messages. Codex can combine these to produce a concise list of changes. For example, if commit messages were conventional, list each as a bullet under categories (Features, Fixes, etc.). If not, just summarize in a paragraph. This can be in Chinese if the project documentation is Chinese. (Likely yes, since internal docs are Chinese.)
Architecture Diagram: Use AI to describe or illustrate the updated architecture. Since generating an actual image automatically is tricky, we can have Codex output a Mermaid diagram code or textual diagram:
E.g., a Mermaid flowchart or sequence diagram showing components (Frontend, Backend, Database, External API). The prompt could be: â€œUpdate the high-level architecture diagram of the system, in Mermaid syntax, highlighting any new component or changes introduced by this PR. If no major arch change, still present the overall architecture for completeness.â€
Mermaid code in the release notes can render on GitHub (Mermaid is supported in markdown). Alternatively, Codex could output an ASCII art diagram or a textual description of the architecture if Mermaid is not desired.
Another option: if there was already an architecture image in docs, we could attempt to update it, but thatâ€™s complex. We stick to generating a new diagram snippet.
Risk List: Ask Codex to enumerate potential risks or things to watch out for after this change is deployed. For example:
Performance hotspots (e.g., â€œthe new image processing might use more memoryâ€),
Security implications (e.g., â€œadded dependency X, ensure itâ€™s trustedâ€),
Known limitations or edge cases not covered.
If the PR is small, risk list might be short or generic (â€œNo major risks â€“ all changes are internal refactors.â€). Thatâ€™s fine.
This essentially acts as a mini post-merge review from the AI focusing on deployment and maintenance concerns (very useful for release managers).
Codex CLI for Release Notes: We can implement this in a workflow (perhaps triggered on push to main). For instance, a release-notes job that runs after merge:
Use actions/checkout to get the latest main.
Use actions/github-script or a custom script to gather data: PR title/body (from the merge commit message or by calling GitHub API to get the merged PR details), commit list in the PR, etc.
Construct a prompt for the OpenAI API (Codex/GPT-4) like:
*â€œYou are an AI Release Notes Assistant. We just merged a pull request into the main branch. Here are the details: [PR title/description, list of commits].
Summarize the changes in a CHANGELOG section (in Chinese) using bullet points.
Provide an Architecture section: give a high-level architecture diagram or description in Mermaid markdown if appropriate.
Provide a Risk Assessment section: list any potential risks or important things to note about this release (performance, security, etc.).â€*
Get the AIâ€™s response and then either:
Use the GitHub API to create a new Release with this content (perhaps tag the commit with a version like v1.0.X if versioning is in place; or use the PR number as an identifier). The release notes field will contain the markdown from AI (which can include the diagram and lists).
Also, possibly commit the changelog: e.g., append to a CHANGELOG.md file and commit. The architecture diagram and risk list could be included in a markdown file in docs/ (since .github/workflows/ci.yml ignores docs/** on push
GitHub
, adding docs wonâ€™t retrigger CI). For persistent record, adding to the repo is nice; but it does create an extra commit on main for every PR merge. If thatâ€™s acceptable, we can do it. Alternatively, rely on GitHub Releases as the source of truth for release notes.
The user explicitly said â€œç”Ÿæˆ CHANGELOGã€æ¶æ„å›¾ã€é£é™©æ¸…å•ç­‰å‘å¸ƒç‰© (generate CHANGELOG, architecture diagram, risk list and other release artifacts).â€ This suggests these might be assembled into release documentation. A GitHub Release page is a logical place. However, it might also be intended to store in the repo for reference. We could do both (generate in release, and commit a copy in docs). Environment & Security: To generate release notes, we again use the AI API keys. Since this runs on push to main (a trusted context), secrets are available. No user input here, just code and commit info, so prompt injection isnâ€™t a concern. We must ensure the AI doesnâ€™t hallucinate incorrect details â€“ providing it with the actual commit diff or PR body helps accuracy. Also, verify the architecture diagram syntax (Mermaid) is correct, or it might not render â€“ possibly have a step to lint the Mermaid (there are CLI tools) or just trust the AI if the architecture is simple. AGENTS.md Updates: In AGENTS.md, add a section for Release Artifact Generation guidelines for Codex:
The AI should be instructed to follow a standard format for release notes: e.g. â€œ## Changelogâ€, â€œ## Architectureâ€, â€œ## Risksâ€ sections.
It should keep the language concise and factual. If writing in Chinese (since the project docs are largely Chinese), maintain a professional tone.
For the architecture diagram, note that it should be high-level (donâ€™t try to include every file, just major components) and accurately reflect any new modules or integration introduced by the PR.
For risks, instruct the agent to think like a reviewer: any security implications, performance concerns, edge cases, or things that need monitoring post-deployment should be listed.
Also, ensure the agent does not reveal sensitive info (like internal server IPs or keys) in release notes. Only discuss what's relevant from the code changes.
Finally, after this step, the PR is merged, a new release notes is generated, and the CI/CD pipeline for that change is essentially complete. The Docker build workflow (already existing) will pick up the new commit on main and build/push the container image to GHCR
GitHub
GitHub
. A maintainer can then trigger the deployment workflow manually to push to production when ready (unless we automate that too in the future).
6. Optimizing Workflows & Documentation (AGENTS.md)
To implement the above, we should refine some existing workflow files and the AGENTS.md guidelines:
CI Workflow (ci.yml): Add the PR auto-create job at the end on success. Also, ensure that the CI jobs have appropriate conditions (we might skip E2E on pushes to non-main to save time, but since we want full test before PR, we kept it manual local. However, in our case we are running tests on push anyway then PR again â€“ a bit redundant. We could decide to run full tests only on PR to avoid double load. But the user specifically said local tests pass then PR, so likely the push is after they've run tests locally. We can keep CI on push to do at least a quick check or just trust local and directly PR. It might be simpler to trigger PR creation immediately on push (assuming the developer only pushes after passing tests). For safety, we used CI to gate it though.) Also, if there are any lint or type-check steps to add (the CICD_SETUP.md mentioned lint/typecheck gates
GitHub
), we should include those before tests and also feed failures to AI fix if needed (though AI fix focusing on test failures primarily; lint issues could also be auto-fixed with a formatter or simple suggestions).
Codex Review Workflow: We might rename it or comment in it that it uses Claude under the hood (to avoid confusion, since itâ€™s called Codex but uses claude-code-action). No functional change needed besides making sure the prompt stays up-to-date with project focus. Given we now have a separate Claude review, we might tweak Codexâ€™s prompt to avoid too much overlap. For example, let Codex focus more on maintainability/testing and Claude on security/compliance, etc. But some redundancy is fine (two opinions on same areas).
ClaudeCode Review Workflow: Itâ€™s quite elaborate already. Ensure the anthropic_base_url handling is only via env (to avoid warning)
GitHub
. The final verification report suggests removing the unsupported input to clean up warnings
GitHub
. We implement that. Also, the continue-on-error: ... trick they used
GitHub
 could be kept or adjusted. (They set Claude step to continue-on-error on first run attempt so that if Claude fails, it doesnâ€™t block CI â€“ it posts fallback instead. Thatâ€™s fine.)
Auto-Fix Workflow: This is new; we will document it clearly. Key points: only runs on our CI failure for PRs, uses secrets for AI, and pushes commits. We will note in the workflow file comments that it implements the steps from AI Bug Fix in the docs
GitHub
.
Merge/Release Workflow: We might integrate this with the Claude review workflow as mentioned. Or implement a small separate workflow triggered on push to main (to handle release notes after every merge). Either approach, clearly comment what it does. Perhaps a separate .github/workflows/release-notes.yml triggered on push to main (filtering out dependabot or docs changes if needed):
Trigger: on: push: branches: [main].
Condition: maybe only trigger when the push is a merge commit from a PR (we can detect commit message contains â€œMerge pull requestâ€).
Steps: checkout, collect PR info via GitHub API (the commit or PR number might be in the commit message), generate notes with AI, publish notes (commit or release).
This decouples it from PR context and ensures it runs in an environment where the code is final.
AGENTS.md: Update this file to reflect:
Codex (OpenAI) Agent: roles in auto-fixing and merging. Provide instructions for how it should handle code changes (as discussed, minimal changes, test focus, commit message conventions). Also guidelines for generating release docs (structure, accuracy, no leaking secrets).
Claude Code Agent: clarify its role is solely review; ensure it doesnâ€™t attempt to make code changes. It should focus on identifying issues. Also mention that its approval will be used to gate auto-merge, so it should only approve if truly no major issues remain. This might entail instructing Claude to treat any test failure or critical security issue as a â€œæ‹’ç»â€ or â€œneeds changesâ€, and only give â€œæ‰¹å‡†â€ if code is production-ready. (The prompt already leans that way by asking for block/approve at end
GitHub
 and a score.)
Perhaps add a note that both agents should remain respectful and professional in comments (they do, since prompt is formal).
Keep existing content about keeping changes small, respecting structure
GitHub
, since thatâ€™s relevant for AI as well (AI should not refactor whole codebase in an auto-fix).
If any environment-specific instructions for AI (like â€œdonâ€™t reveal API keysâ€, or â€œif something about DeepSeek API appears, note itâ€), include those.
Summary: With the above changes, the CI/CD pipeline becomes fully automated:
A developerâ€™s push triggers CI and automatically opens a PR if tests passed.
Opening a PR triggers both Codex and ClaudeCode AI reviews independently
GitHub
GitHub
, as well as the CI tests on the PR.
If CI fails, an AI fix workflow activates to correct the code and push a commit
GitHub
. The PR is updated and CI runs again.
Each update triggers the AI reviews again, providing iterative feedback until no critical issues remain.
Once CI is green and AI reviews signal approval, the pipeline automatically merges the PR. Codex then generates the release artifacts: updating the CHANGELOG and producing an architecture diagram and risk assessment for the release (e.g., via a Release note or commit).
The Docker image build workflow picks up the new main commit and builds/deploys as configured (continuing the CD process)
GitHub
.
This end-to-end pipeline fulfills the userâ€™s requirements. It leverages the existing workflows (CI, Claude review) and adds the missing pieces (auto PR, auto-fix, auto-merge, and release note generation) in a cohesive manner. The result is a highly automated CI/CD system where AI agents assist at each stage: code review, test failure fixing, and even documenting the release. Human oversight is still important (one should monitor AI commits and reviews), but the heavy lifting is automated, accelerating the development workflow. Sources:
StoryApp CI workflow (build/tests)
GitHub
GitHub
Codex review workflow (Claude-based, PR trigger, prompt with ğŸ”´ğŸŸ¡ğŸŸ¢ and merge suggestion)
GitHub
GitHub
ClaudeCode auto-review workflow (PR trigger, structured review report template)
GitHub
GitHub
Project docs â€“ AI bug fix flow and CI/CD overview
GitHub
GitHub
Final fix report â€“ Claude action integration and best practices (token usage, env vars, continue-on-error)
GitHub
GitHub
Repository guidelines (commit conventions, PR requirements)
GitHub
 and environment configuration for AI
GitHub
.
Citations
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L3-L11
GitHub
AGENTS.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/AGENTS.md#L80-L88
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L3-L11
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L3-L11
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L34-L42
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L40-L48
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L50-L54
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L14-L22
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L7-L14
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L16-L24
GitHub
codex-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/codex-review.yml#L20-L27
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L44-L53
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L56-L64
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L50-L58
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L66-L75
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L76-L84
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L34-L42
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L16-L24
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L22-L29
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L8-L16
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L107-L116
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L126-L134
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L160-L168
GitHub
README.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/README.md#L46-L55
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L116-L124
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L84-L93
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L84-L92
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L16-L24
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L100-L109
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L36-L44
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L48-L56
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L61-L69
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L88-L97
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L100-L108
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L106-L114
GitHub
CLAUDE.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CLAUDE.md#L86-L94
GitHub
README.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/README.md#L112-L120
GitHub
README.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/README.md#L112-L115
GitHub
README.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/README.md#L84-L88
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L94-L101
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L14-L17
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L76-L79
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L160-L169
GitHub
pr-auto-review.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/pr-auto-review.yml#L112-L120
GitHub
CICD_SETUP.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CICD_SETUP.md#L59-L67
GitHub
CICD_SETUP.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CICD_SETUP.md#L94-L100
GitHub
CICD_SETUP.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CICD_SETUP.md#L20-L28
GitHub
ci.yml

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/.github/workflows/ci.yml#L6-L14
GitHub
CICD_SETUP.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CICD_SETUP.md#L64-L67
GitHub
CICD_SETUP.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/CICD_SETUP.md#L14-L18
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L120-L124
GitHub
AGENTS.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/AGENTS.md#L95-L99
GitHub
FINAL_VERIFICATION_REPORT.md

https://github.com/haizhouyuan/storyapp/blob/5b735eb8225842d2ab1b2ff9639a1f635c79dae9/FINAL_VERIFICATION_REPORT.md#L32-L40
All Sources

github