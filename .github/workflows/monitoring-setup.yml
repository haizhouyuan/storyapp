name: Enhanced Monitoring Setup

on:
  workflow_run:
    workflows: ["Auto Fix on Comment", "ClaudeCode Autofix on CI failure", "Security Auto-Fix"]
    types: [completed]
  workflow_dispatch:
    inputs:
      collect_historical:
        description: 'æ”¶é›†å†å²æ•°æ®'
        required: false
        default: false
        type: boolean
      analysis_period_days:
        description: 'åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰'
        required: false
        default: '30'
        type: string

env:
  ANALYSIS_DAYS: ${{ inputs.analysis_period_days || '7' }}

jobs:
  collect-metrics:
    runs-on: ubuntu-latest
    outputs:
      metrics_collected: ${{ steps.collect.outputs.metrics_available }}
      analysis_report: ${{ steps.analyze.outputs.report_file }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests PyGithub python-dateutil matplotlib seaborn pandas numpy

      - name: Collect Fix Metrics
        id: collect
        run: |
          echo "=== æ”¶é›†ä¿®å¤æŒ‡æ ‡ ==="

          # åˆ›å»ºæŒ‡æ ‡æ”¶é›†ç›®å½•
          mkdir -p monitoring-data/raw
          mkdir -p monitoring-data/processed
          mkdir -p monitoring-data/reports

          # æ”¶é›†å½“å‰å·¥ä½œæµæŒ‡æ ‡
          cat > monitoring-data/raw/current-workflow.json << 'EOF'
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "workflow_id": "${{ github.run_id }}",
            "workflow_name": "${{ github.workflow }}",
            "trigger_type": "${{ github.event_name }}",
            "repository": "${{ github.repository }}",
            "branch": "${{ github.ref_name }}",
            "actor": "${{ github.actor }}",
            "run_attempt": "${{ github.run_attempt }}",
            "triggering_workflow": {
              "name": "${{ github.event.workflow_run.name || 'N/A' }}",
              "conclusion": "${{ github.event.workflow_run.conclusion || 'N/A' }}",
              "started_at": "${{ github.event.workflow_run.created_at || 'N/A' }}",
              "completed_at": "${{ github.event.workflow_run.updated_at || 'N/A' }}"
            }
          }
          EOF

          # ä½¿ç”¨Pythonè„šæœ¬æ”¶é›†è¯¦ç»†æŒ‡æ ‡
          python3 scripts/metrics-calculator.py \
            --workflow-id="${{ github.run_id }}" \
            --repository="${{ github.repository }}" \
            --analysis-days="${{ env.ANALYSIS_DAYS }}" \
            --output-dir="monitoring-data/raw" \
            --github-token="${{ secrets.GITHUB_TOKEN }}"

          # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°æ•°æ®
          if [ -f "monitoring-data/raw/workflow-metrics.json" ]; then
            echo "metrics_available=true" >> $GITHUB_OUTPUT
            echo "âœ… æŒ‡æ ‡æ”¶é›†å®Œæˆ"
          else
            echo "metrics_available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  æŒ‡æ ‡æ”¶é›†ä¸å®Œæ•´"
          fi

      - name: Analyze Metrics
        id: analyze
        if: steps.collect.outputs.metrics_available == 'true'
        run: |
          echo "=== åˆ†æä¿®å¤æŒ‡æ ‡ ==="

          # è¿è¡ŒæŒ‡æ ‡åˆ†æ
          python3 -c "
          import json
          import os
          from datetime import datetime, timedelta
          import statistics

          # è¯»å–æ”¶é›†çš„æŒ‡æ ‡
          metrics_files = []
          for root, dirs, files in os.walk('monitoring-data/raw'):
              for file in files:
                  if file.endswith('.json'):
                      metrics_files.append(os.path.join(root, file))

          all_metrics = []
          for file_path in metrics_files:
              try:
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      if isinstance(data, list):
                          all_metrics.extend(data)
                      else:
                          all_metrics.append(data)
              except Exception as e:
                  print(f'Error reading {file_path}: {e}')

          # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
          analysis = {
              'analysis_timestamp': datetime.utcnow().isoformat(),
              'analysis_period_days': ${{ env.ANALYSIS_DAYS }},
              'total_workflows': len(all_metrics),
              'workflow_types': {},
              'success_metrics': {
                  'total_attempts': 0,
                  'successful_attempts': 0,
                  'success_rate': 0.0
              },
              'performance_metrics': {
                  'avg_duration_minutes': 0.0,
                  'duration_percentiles': {},
                  'trend_analysis': 'stable'
              },
              'error_analysis': {
                  'common_errors': {},
                  'error_rate': 0.0,
                  'resolution_time': {}
              },
              'recommendations': []
          }

          # åˆ†æå·¥ä½œæµç±»å‹
          for metric in all_metrics:
              workflow_name = metric.get('workflow_name', 'unknown')
              analysis['workflow_types'][workflow_name] = analysis['workflow_types'].get(workflow_name, 0) + 1

          # è®¡ç®—æˆåŠŸç‡
          successes = sum(1 for m in all_metrics if m.get('triggering_workflow', {}).get('conclusion') == 'success')
          total = len([m for m in all_metrics if m.get('triggering_workflow', {}).get('conclusion') in ['success', 'failure']])

          if total > 0:
              analysis['success_metrics']['total_attempts'] = total
              analysis['success_metrics']['successful_attempts'] = successes
              analysis['success_metrics']['success_rate'] = successes / total

          # ç”Ÿæˆå»ºè®®
          if analysis['success_metrics']['success_rate'] < 0.8:
              analysis['recommendations'].append('æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥è‡ªåŠ¨ä¿®å¤é€»è¾‘')

          if analysis['total_workflows'] < 5:
              analysis['recommendations'].append('æ ·æœ¬æ•°æ®è¾ƒå°‘ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥è·å¾—å‡†ç¡®åˆ†æ')

          # ä¿å­˜åˆ†æç»“æœ
          with open('monitoring-data/processed/analysis-report.json', 'w') as f:
              json.dump(analysis, f, indent=2)

          print(f'åˆ†æå®Œæˆ: {analysis[\"total_workflows\"]} ä¸ªå·¥ä½œæµè®°å½•')
          print(f'æˆåŠŸç‡: {analysis[\"success_metrics\"][\"success_rate\"]:.1%}')
          "

          if [ -f "monitoring-data/processed/analysis-report.json" ]; then
            echo "report_file=monitoring-data/processed/analysis-report.json" >> $GITHUB_OUTPUT
            echo "âœ… æŒ‡æ ‡åˆ†æå®Œæˆ"
          else
            echo "âš ï¸  æŒ‡æ ‡åˆ†æå¤±è´¥"
          fi

      - name: Generate Dashboard Data
        if: steps.analyze.outputs.report_file
        run: |
          echo "=== ç”ŸæˆDashboardæ•°æ® ==="

          python3 scripts/setup-monitoring-dashboard.py \
            --analysis-report="${{ steps.analyze.outputs.report_file }}" \
            --output-dir="monitoring-data/dashboard" \
            --format="json"

          echo "âœ… Dashboardæ•°æ®ç”Ÿæˆå®Œæˆ"

      - name: Upload Monitoring Data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-data-${{ github.run_id }}
          path: monitoring-data/
          retention-days: 90

      - name: Generate Summary Report
        if: steps.analyze.outputs.report_file
        run: |
          echo "=== ç›‘æ§æ‘˜è¦æŠ¥å‘Š ==="

          python3 -c "
          import json

          try:
              with open('${{ steps.analyze.outputs.report_file }}', 'r') as f:
                  analysis = json.load(f)

              print('ğŸ“Š AutoFixç³»ç»Ÿç›‘æ§æ‘˜è¦')
              print('=' * 40)
              print(f'åˆ†ææ—¶é—´èŒƒå›´: {analysis.get(\"analysis_period_days\", \"N/A\")} å¤©')
              print(f'å·¥ä½œæµæ‰§è¡Œæ¬¡æ•°: {analysis.get(\"total_workflows\", 0)}')

              success_metrics = analysis.get('success_metrics', {})
              print(f'ä¿®å¤æˆåŠŸç‡: {success_metrics.get(\"success_rate\", 0):.1%}')
              print(f'æˆåŠŸæ¬¡æ•°: {success_metrics.get(\"successful_attempts\", 0)}/{success_metrics.get(\"total_attempts\", 0)}')

              workflow_types = analysis.get('workflow_types', {})
              if workflow_types:
                  print('\\nå·¥ä½œæµç±»å‹åˆ†å¸ƒ:')
                  for wf_type, count in workflow_types.items():
                      print(f'  {wf_type}: {count}')

              recommendations = analysis.get('recommendations', [])
              if recommendations:
                  print('\\nğŸ“‹ æ”¹è¿›å»ºè®®:')
                  for i, rec in enumerate(recommendations, 1):
                      print(f'  {i}. {rec}')

          except Exception as e:
              print(f'ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šæ—¶å‡ºé”™: {e}')
          "

  send-notifications:
    needs: collect-metrics
    if: needs.collect-metrics.outputs.metrics_collected == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Download Monitoring Data
        uses: actions/download-artifact@v4
        with:
          name: monitoring-data-${{ github.run_id }}
          path: monitoring-data/

      - name: Send Performance Alert
        if: always()
        run: |
          echo "=== æ£€æŸ¥æ€§èƒ½å‘Šè­¦æ¡ä»¶ ==="

          # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å‘Šè­¦
          SEND_ALERT=false
          ALERT_MESSAGE=""

          if [ -f "monitoring-data/processed/analysis-report.json" ]; then
            SUCCESS_RATE=$(python3 -c "
            import json
            try:
                with open('monitoring-data/processed/analysis-report.json', 'r') as f:
                    data = json.load(f)
                print(data.get('success_metrics', {}).get('success_rate', 1.0))
            except:
                print(1.0)
            ")

            # æ£€æŸ¥æˆåŠŸç‡é˜ˆå€¼
            if [ "$(echo "$SUCCESS_RATE < 0.7" | bc -l 2>/dev/null || echo "0")" = "1" ]; then
              SEND_ALERT=true
              ALERT_MESSAGE="ğŸš¨ AutoFixæˆåŠŸç‡å‘Šè­¦: ${SUCCESS_RATE}% < 70%"
            fi

            # æ£€æŸ¥å…¶ä»–å‘Šè­¦æ¡ä»¶
            TOTAL_WORKFLOWS=$(python3 -c "
            import json
            try:
                with open('monitoring-data/processed/analysis-report.json', 'r') as f:
                    data = json.load(f)
                print(data.get('total_workflows', 0))
            except:
                print(0)
            ")

            if [ "$TOTAL_WORKFLOWS" -gt 20 ]; then
              echo "âš ï¸  å·¥ä½œæµæ‰§è¡Œé¢‘ç‡è¾ƒé«˜: $TOTAL_WORKFLOWS æ¬¡"
            fi
          fi

          if [ "$SEND_ALERT" = "true" ]; then
            echo "ğŸš¨ éœ€è¦å‘é€å‘Šè­¦: $ALERT_MESSAGE"

            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼š
            # - Slack webhook
            # - é‚®ä»¶é€šçŸ¥
            # - é’‰é’‰æœºå™¨äºº
            # - ä¼ä¸šå¾®ä¿¡

            # ç¤ºä¾‹ï¼šå‘é€åˆ°GitHub Issueï¼ˆä½œä¸ºå‘Šè­¦è®°å½•ï¼‰
            echo "åˆ›å»ºå‘Šè­¦Issue..."

          else
            echo "âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ— éœ€å‘Šè­¦"
          fi

      - name: Update Dashboard
        run: |
          echo "=== æ›´æ–°ç›‘æ§Dashboard ==="

          # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„Dashboardç³»ç»Ÿï¼Œä¾‹å¦‚ï¼š
          # - Grafana
          # - Kibana
          # - è‡ªå»ºç›‘æ§é¡µé¢
          # - äº‘æœåŠ¡ç›‘æ§

          if [ -d "monitoring-data/dashboard" ]; then
            echo "ğŸ“Š Dashboardæ•°æ®å·²å‡†å¤‡å°±ç»ª"
            echo "æ•°æ®æ–‡ä»¶ï¼š"
            find monitoring-data/dashboard -type f -name "*.json" | head -5

            # ç¤ºä¾‹ï¼šä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨æˆ–æ¨é€åˆ°ç›‘æ§ç³»ç»Ÿ
            echo "ğŸ’¾ Dashboardæ•°æ®å¯ä»¥æ¨é€åˆ°ç›‘æ§ç³»ç»Ÿ"
          fi

  cleanup-old-data:
    needs: [collect-metrics, send-notifications]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Cleanup Old Monitoring Data
        run: |
          echo "=== æ¸…ç†æ—§çš„ç›‘æ§æ•°æ® ==="

          # æ¸…ç†è¶…è¿‡30å¤©çš„å·¥ä½œæµæ•°æ®
          # è¿™é‡Œæ˜¯ç¤ºä¾‹é€»è¾‘ï¼Œå®é™…å¯ä»¥è¿æ¥æ•°æ®åº“æˆ–å­˜å‚¨ç³»ç»Ÿ

          echo "ğŸ§¹ æ¸…ç†ç­–ç•¥:"
          echo "  - ä¿ç•™æœ€è¿‘30å¤©çš„è¯¦ç»†æ•°æ®"
          echo "  - ä¿ç•™æœ€è¿‘90å¤©çš„æ‘˜è¦æ•°æ®"
          echo "  - ä¿ç•™æœ€è¿‘1å¹´çš„è¶‹åŠ¿æ•°æ®"

          # ç¤ºä¾‹æ¸…ç†å‘½ä»¤
          find . -name "monitoring-data-*" -mtime +30 -type d -exec rm -rf {} \; 2>/dev/null || true

          echo "âœ… æ•°æ®æ¸…ç†å®Œæˆ"

  generate-monthly-report:
    if: github.event_name == 'workflow_dispatch' && inputs.collect_historical == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Generate Monthly Report
        run: |
          echo "=== ç”Ÿæˆæœˆåº¦æŠ¥å‘Š ==="

          # ç”Ÿæˆæœˆåº¦AutoFixç³»ç»ŸæŠ¥å‘Š
          MONTH=$(date +%Y-%m)

          echo "ğŸ“ˆ $MONTH AutoFixç³»ç»Ÿæœˆåº¦æŠ¥å‘Š"
          echo "=" * 50

          echo "ğŸ“Š å…³é”®æŒ‡æ ‡:"
          echo "  - æ€»ä¿®å¤å°è¯•: å¾…ç»Ÿè®¡"
          echo "  - ä¿®å¤æˆåŠŸç‡: å¾…ç»Ÿè®¡"
          echo "  - å¹³å‡ä¿®å¤æ—¶é—´: å¾…ç»Ÿè®¡"
          echo "  - é”™è¯¯ç±»å‹åˆ†å¸ƒ: å¾…ç»Ÿè®¡"

          echo ""
          echo "ğŸ“‹ æ”¹è¿›å»ºè®®:"
          echo "  - å¾…åˆ†æç”Ÿæˆ"

          echo ""
          echo "ğŸ¯ ä¸‹æœˆç›®æ ‡:"
          echo "  - æå‡ä¿®å¤æˆåŠŸç‡è‡³85%+"
          echo "  - é™ä½å¹³å‡ä¿®å¤æ—¶é—´"
          echo "  - æ‰©å±•æ”¯æŒçš„é”™è¯¯ç±»å‹"

          # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–æ—¥å¿—ç³»ç»Ÿ
          # æ”¶é›†çœŸå®çš„å†å²æ•°æ®å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š

workflow_summary:
  needs: [collect-metrics, send-notifications, cleanup-old-data]
  if: always()
  runs-on: ubuntu-latest

  steps:
    - name: Workflow Summary
      run: |
        echo "=== ç›‘æ§å·¥ä½œæµæ‰§è¡Œæ‘˜è¦ ==="
        echo "æŒ‡æ ‡æ”¶é›†: ${{ needs.collect-metrics.result }}"
        echo "é€šçŸ¥å‘é€: ${{ needs.send-notifications.result }}"
        echo "æ•°æ®æ¸…ç†: ${{ needs.cleanup-old-data.result }}"
        echo ""

        if [ "${{ needs.collect-metrics.result }}" = "success" ]; then
          echo "âœ… ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ"
        else
          echo "âŒ ç›‘æ§æ•°æ®æ”¶é›†å¤±è´¥"
        fi

        echo ""
        echo "ğŸ“Š ç›‘æ§ç³»ç»ŸçŠ¶æ€: æ­£å¸¸è¿è¡Œ"
        echo "ğŸ“… ä¸‹æ¬¡æ£€æŸ¥: ä¸‹ä¸ªå·¥ä½œæµæ‰§è¡Œæ—¶"